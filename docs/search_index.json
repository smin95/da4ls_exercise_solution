[
["index.html", "Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Welcome! Acknowledgment Frequently Asked Questions", " Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Seung Hyun (Sam) Min 2020-12-12 Welcome! This book contains unofficial exercise solutions for the book Data Analysis for the Life Sciences with R by Rafael A. Irizarry and Michael I. Love. The PDF copy of the book is available for free and the physical copy is available in Amazon. Acknowledgment I would like to thank Rafael A. Irizarry and Michael I. Love for writing this wonderful book, and my friends who encouraged me to undertake this project. Frequently Asked Questions You can read the FAQs in the github page. "],
["getting-started.html", "Chapter 1 Getting started", " Chapter 1 Getting started Since this chapter does not deal with statistics, I have decided to skip this chapter altogether. Before reading DA4LS with R The book gives a brief introduction to R. But I do not think this is enough. The codes in the later part of the book can get quite complicated (especially from Chapter 6). If you do not know R (or have no coding experience), I suggest you read Chapters 5, 15, 19-21 of R for Data Science by Hadley Wickham and Garrett Grolemund; these chapters discuss data transformation, factors, custom functions, vectors and iterations (ex. for loops, sapply). These concepts are deeply embeded in the codes throughout the book. There is also matrix algebra (ex. singular value decomposition) in this book (from Chapter 4). However, the book reviews matrix algebra briefly. If you have not taken a course in linear algebra, I suggest that you spend some extra time reviewing key concepts such as matrix multiplication, dot product, orthogonal matrix, inverse matrix and square matrix before reading Chapter 4. Here is a link to Khan Academy. When you are going through Chapter 8, you might also have you look up for extra resource on principal component analysis; I recommend this Youtube video by StatQuest. "],
["inference.html", "Chapter 2 Inference 2.7 Exercises 2.9 Exercises 2.11 Exercises 2.13 Exercises 2.18 Exercises 2.21 Exercises 2.23 Exercises 2.25 Exercises", " Chapter 2 Inference Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the functions filter() and %&gt;% library(rafalib) # important for plotting with base R 2.7 Exercises If you have not downloaded the data before, dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleControlsPopulation.csv&quot; url &lt;- paste0(dir, filename) x &lt;- unlist(read.csv(url)) Or if you already have downloaded the data, then just upload it. dat &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) bodyweight &lt;- dplyr::select(dat, Bodyweight) x &lt;- unlist(bodyweight) # or use pipe %&gt;% x &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() Check out what unlist does by typing ?unlist in the command. The second method is more concise because of the pipe %&gt;%, which allows multiple lines of commands to be in one continuous line. Question 1 What is the average of these weights? mean(x) ## [1] 23.89338 Question 2 After setting the seed at 1, set.seed(1) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(1) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 0.2706222 Question 3 After setting the seed at 5, set.seed(5) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(5) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 1.433378 Question 4 Why are the answers from 2 and 3 different? set.seed(1) # question 2 a &lt;- sample(x,5) a ## Bodyweight60 Bodyweight84 Bodyweight128 Bodyweight202 ## 21.51 28.14 24.04 23.45 ## Bodyweight45 ## 23.68 set.seed(5) # question 3 b &lt;- sample(x,5) b ## Bodyweight46 Bodyweight154 Bodyweight205 Bodyweight64 ## 21.86 20.30 22.95 21.92 ## Bodyweight24 ## 25.27 identical(a,b) # these two samples are not identical ## [1] FALSE Notice that samples a and b differ. Since the seeds were different (1 vs 5), different random numbers were generated. Therefore, the answer is C: Because the average of the samples is a random variable. Question 5 Set the seed at 1, then using a for-loop take a random sample of 5 mice in 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.498 To make a for loop work in R, an empty vector needs to be created first. This can be achieved with the function vector. In this example, the empty vector is res (short for result). In the for loop, each average (avg_sample) from one repetition gets stored in res. Question 6 We are now going to increase the number of times we redo the sample from 1,000 to 10,000. Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save these averages. What percent of these 10,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 10000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.4976 Question 7 Note that the answers to 5 and 6 barely changed. This is expected. The way we think about the random value distributions is as the distribution of the list of values obtained if we repeated the experiment an infinite number of times. On a computer, we can’t perform an infinite number of iterations so instead, for our examples, we consider 1,000 to be large enough, thus 10,000 is as well. Now if instead we change the sample size, then we change the random variable and thus its distribution. Set the seed at 1, then using a for-loop take a random sample of 50 mice 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.019 Question 8 Use a histogram to “look” at the distribution of averages we get with a sample size of 5 and sample size of 50. How would you say they differ? # sample size = 5 set.seed(1) n &lt;- 1000 res5 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res5[[i]] &lt;- avg_sample } sd(res5) # standard deviation = spread of the histogram ## [1] 1.52445 # sample size = 50 set.seed(1) n &lt;- 1000 res50 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res50[[i]] &lt;- avg_sample } sd(res50) # standard deviation = spread of the histogram ## [1] 0.4260116 The standard deviation of res50 is smaller than that of res5 because of the difference in the sample sizes. A higher standard deviation leads to a wider histogram. See two figures below. mypar(1,2) # plot histograms hist(res5) hist(res50) mypar is a function from the package rafalib. It helps to align multiple plots in a single plot. mypar(1,1) contains one panel only, mypar(2,1) contains 2 rows of panels and 1 column, mypar(1,2) contains 1 row of panels and 2 columns, etc. Type ?mypar for more information. You will be using this function to plot a graph throughout the entire book. hist plots a histogram. The answer is B: They both look normal, but with a sample size of 50 the spread is smaller. Question 9 For the last set of averages, the ones obtained from a sample size of 50, what percent are between 23 and 25? mean((res50 &gt;=23) &amp; (res50 &lt;= 25)) ## [1] 0.976 Question 10 Now ask the same question of a normal distribution with average 23.9 and standard deviation 0.43. pnorm(25,23.9,0.43) - pnorm(23,23.9,0.43) ## [1] 0.9765648 The answers to 9 and 10 were very similar. This is because we can approximate the distribution of the sample average with a normal distribution. We will learn more about the reason for this next. 2.9 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) Question 1 Use dplyr to create a vector x with the body weight of all males on the control (chow) diet. What is this population’s average? x &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() mean(x) ## [1] 30.96381 Throughout the book, I will be using %&gt;% for brevity. If you don’t understand it, please check out Chapter 18 of *R for Data Science. Question 2 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(x) ## [1] 4.420501 popsd and sd are slightly different. sd calculates the standard deviation of the sample size, so the denominator that it uses to compute SD is n-1. Function var also uses denominator n-1 to calculate variance. However, popsd (which is from rafalib package) uses denominator n. Question 3 Set the seed at 1. Take a random sample X of size 25 from x. What is the sample average? set.seed(1) samp_x &lt;- sample(x,25) # sample of x mean(samp_x) ## [1] 32.0956 Question 4 Use dplyr to create a vector y with the body weight of all males on the high fat (hf) diet. What is this population’s average? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() mean(y) ## [1] 34.84793 Question 5 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(y) ## [1] 5.574609 Question 6 Set the seed at 1. Take a random sample Y of size 25 from y. What is the sample average? set.seed(1) samp_y &lt;- sample(y,25) mean(samp_y) ## [1] 34.768 Question 7 What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? pop_diff &lt;- mean(y) - mean(x) sample_diff &lt;- mean(samp_y) - mean(samp_x) abs(sample_diff - pop_diff) ## [1] 1.211716 Question 8 Repeat the above for females. Make sure to set the seed to 1 before each sample call. What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? chow_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() # x hf_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() # y set.seed(1) sample_chow_f_pop &lt;- sample(chow_f_pop, 25) # X set.seed(1) sample_hf_f_pop &lt;- sample(hf_f_pop,25) # Y pop_diff &lt;- mean(hf_f_pop) - mean(chow_f_pop) # y - x sample_diff &lt;- mean(sample_hf_f_pop) - mean(sample_chow_f_pop) # Y - X abs(sample_diff - pop_diff) ## [1] 0.7364828 Question 9 For the females, our sample estimates were closer to the population difference than with males. What is a possible explanation for this? ans &lt;- c(popsd(hf_f_pop), popsd(chow_f_pop), popsd(y), popsd(x)) names(ans) &lt;- c(&#39;hf female&#39;, &#39;chow female&#39;, &#39;hf male&#39;, &#39;chow male&#39;) ans ## hf female chow female hf male chow male ## 5.069870 3.416438 5.574609 4.420501 The answer is A: The population variance of the females is smaller than that of the males; thus, the sample variable has less variability. 2.11 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) Question 1 If a list of numbers has a distribution that is well approximated by the normal distribution, what proportion of these numbers are within one standard deviation away from the list’s average? pnorm(1) - pnorm(-1) ## [1] 0.6826895 Question 2 What proportion of these numbers are within two standard deviations away from the list’s average? pnorm(2) - pnorm(-2) ## [1] 0.9544997 Question 3 What proportion of these numbers are within three standard deviations away from the list’s average? pnorm(3) - pnorm(-3) ## [1] 0.9973002 Question 4 Define y to be the weights of males on the control diet. What proportion of the mice are within one standard deviation away from the average weight (remember to use popsd for the population sd)? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() z_score &lt;- (y - mean(y))/popsd(y) # get t-statistic (i.e., z score) mean(abs(z_score) &lt;= 1) ## [1] 0.6950673 mean(abs(y - mean(y)) &lt;= popsd(y)) ## [1] 0.6950673 It doesn’t matter which solution you use as long as you have the same answer. Question 5 What proportion of these numbers are within two standard deviations away from the list’s average? mean(abs(z_score) &lt;= 2) ## [1] 0.9461883 mean(abs(y - mean(y)) &lt;= 2*popsd(y)) ## [1] 0.9461883 It doesn’t matter which solution you use as long as you have the same answer. Question 6 What proportion of these numbers are within three standard deviations away from the list’s average? mean(abs(z_score) &lt;= 3) ## [1] 0.9910314 mean(abs(y - mean(y)) &lt;= 3*popsd(y)) ## [1] 0.9910314 It doesn’t matter which solution you use as long as you have the same answer. Question 7 Note that the numbers for the normal distribution and our weights are relatively close. Also, notice that we are indirectly comparing quantiles of the normal distribution to quantiles of the mouse weight distribution. We can actually compare all quantiles using a qq-plot. Which of the following best describes the qq-plot comparing mouse weights to the normal distribution? mypar(1,1) qqnorm(y) qqline(y) The answer is C: The mouse weights are well approximated by the normal distribution, although the larger values (right tail) are larger than predicted by the normal.This is consistent with the differences seen between question 3 and 6. Question 8 Create the above qq-plot for the four populations: male/females on each of the two diets. What is the most likely explanation for the mouse weights being well approximated? What is the best explanation for all these being well approximated by the normal distribution? mc &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() mhf &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() fc &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() fhf &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist() mypar(2,2) qqnorm(mc, main = &#39;male control pop&#39;) qqline(mc) qqnorm(mhf, main = &#39;male high fat pop&#39;) qqline(mhf) qqnorm(fc, main = &#39;female control pop&#39;) qqline(fc) qqnorm(fhf, main = &#39;female high fat pop&#39;) qqline(fhf) The answer is B: This just happens to be how nature behaves. Perhaps the result of many biological factors averaging out. Question 9 Here we are going to use the function replicate to learn about the distribution of random variables. All the above exercises relate to the normal distribution as an approximation of the distribution of a fixed list of numbers or a population. We have not yet discussed probability in these exercises. If the distribution of a list of numbers is approximately normal, then if we pick a number at random from this distribution, it will follow a normal distribution. However, it is important to remember that stating that some quantity has a distribution does not necessarily imply this quantity is random. Also, keep in mind that this is not related to the central limit theorem. The central limit applies to averages of random variables. Let’s explore this concept. We will now take a sample of size 25 from the population of males on the chow diet. The average of this sample is our random variable. We will use the replicate to observe 10,000 realizations of this random variable. Set the seed at 1, generate these 10,000 averages. Make a histogram and qq-plot of these 10,000 numbers against the normal distribution. We can see that, as predicted by the CLT, the distribution of the random variable is very well approximated by the normal distribution. y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist avgs &lt;- replicate(10000, mean( sample(y, 25))) mypar(1,2) hist(avgs) qqnorm(avgs) qqline(avgs) What is the average of the distribution of the sample average? m &lt;- 10000 n &lt;- 25 y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist set.seed(1) avg_list &lt;- replicate(m, { mean(sample(y,25)) }) mypar(1,2) hist(avg_list) # distribution qqnorm(avg_list) # qq-plot qqline(avg_list) mean(avg_list) # mean of the sample averages ## [1] 30.95581 Question 10 What is the standard deviation of the distribution of sample averages? popsd(avg_list) ## [1] 0.8368192 Question 11 According to the CLT, the answer to exercise 9 should be the same as mean(y). You should be able to confirm that these two numbers are very close. Which of the following does the CLT tell us should be close to your answer to exercise 10? popsd(y)/sqrt(25) # answer is C ## [1] 0.8841001 Question 12 In practice we do not know \\(\\sigma\\) (popsd(y)) which is why we can’t use the CLT directly. This is because we see a sample and not the entire distribution. We also can’t use popsd(avgs) because to construct averages, we have to take 10,000 samples and this is never practical. We usually just get one sample. Instead we have to estimate popsd(y). As described, what we use is the sample standard deviation. Set the seed at 1, using the replicate function, create 10,000 samples of 25 and now, instead of the sample average, keep the standard deviation. Look at the distribution of the sample standard deviations. It is a random variable. The real population SD is about 4.5. What proportion of the sample SDs are below 3.5? m &lt;- 10000 set.seed(1) sd_list &lt;- replicate(m, { sd(sample(y,25)) }) mypar(1,1) hist(sd_list) mean(sd_list &lt;= 3.5) ## [1] 0.0964 Question 13 What the answer to question 12 reveals is that the denominator of the t-test is a random variable. By decreasing the sample size, you can see how this variability can increase. It therefore adds variability. The smaller the sample size, the more variability is added. The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which of the following is true? x = seq(0.0001, 0.9999, len = 300) df_list &lt;- c(3,10,30,100) mypar(2,2) for (i in seq_along(df_list)) { qqnorm(qt(x,df_list[i]), main = df_list[i]) } The answer is C: The t-distribution has larger tails up until 30 degrees of freedom, at which point it is practically the same as the normal distribution. 2.13 Exercises dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleMiceWeights.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) Question 1 The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let’s try a simple example related to gambling. Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies. We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). n &lt;- 100 B &lt;- 10000 p &lt;- 1/6 set.seed(1) res_list &lt;- replicate(B, { x = sample(1:6,n, replace = T) z &lt;- (mean(x==6)-p) / sqrt(p*(1-p)/n) return(z) }) mean(abs(res_list) &gt; 2) ## [1] 0.0424 Question 2 For the last simulation you can make a qqplot to confirm the normal approximation. Now, the CLT is an asymptotic result, meaning it is closer and closer to being a perfect approximation as the sample size increases. In practice, however, we need to decide if it is appropriate for actual sample sizes. Is 10 enough? 15? 30? In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to “kick in”. Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best? Ps &lt;- c(0.01,0.5) Ns &lt;- c(5,30,100) set.seed(1) question2 &lt;- function(n,p, B = 10000) { res_list &lt;- replicate(B, { sides &lt;- 1/p x = sample(1:sides, n, replace = T) z &lt;- (mean(x==1)-p) / sqrt(p*(1-p)/n) return(z) }) } mypar(2,2) qqnorm(question2(5,0.5), main = &#39;n=5, p=0.5&#39;) qqnorm(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) # the answer is B qqnorm(question2(30,0.01), main = &#39;n=30, p=0.01&#39;) qqnorm(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) mypar(1,2) hist(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) hist(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) The answer is B, n = 30, p = 0.5. I created a custom function for this question. However, another approach (and maybe more concise) is to use a for loop. This is tricky question, primarily because not all dice has number 6 (as is the case in Question 1, mean(x==6)). So you had to calculate the probability where the dice ends up in another side by modifying the code; since all dice has sdie 1, mean(x==1) seems like a safe choice. Also, the number of the sides for each die changes since it is defined by the p value. For instance, if p equals 0.01 for side 1, then there are 100 sides total (1/0.01 = 100). Question 3 As we have already seen, the CLT also applies to averages of quantitative data. A major difference with binary data, for which we know the variance is p(1-p), is that with quantitative data we need to estimate the population standard deviation. In several previous exercises we have illustrated statistical concepts with the unrealistic situation of having access to the entire population. In practice, we do not have access to entire populations. Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations: X &lt;- filter(dat, Diet==&quot;chow&quot;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist Y &lt;- filter(dat, Diet==&quot;hf&quot;) %&gt;% dplyr::select(Bodyweight) %&gt;% unlist We think of X as a random sample from the population of all mice in the control diet and Y as a random sample from the population of all mice in the high fat diet. Define the parameter \\(\\mu_x\\) as the average of the control population. We estimate this parameter with the sample average \\(\\bar{X}\\). What is the sample average? mean(X) ## [1] 23.81333 Question 4 We don’t know \\(\\mu_x\\), but want to use \\(\\bar{X}\\) to understand \\(\\mu_x\\). Which of the following uses CLT to understand how well \\(\\bar{X}\\) approximates \\(\\mu_x\\)? The answer is 0. Z refers to t-statistic (something that the book does not explicitly highlight), which is not raw data. Instead, we use raw data and then compute t-statistic. This is the value that is used to compute p-value based on the normal distribution. In the case of z-score or t-statistic, it is 0 at the mean of the sample due to its mathematical definition. Question 6 The result of 4 and 5 tell us that we know the distribution of the difference between our estimate and what we want to estimate, but don’t know. However, the equation involves the population standard deviation \\(\\sigma_X\\), which we don’t know. Given what we discussed, what is your estimate of \\(\\sigma_x\\)? sd(X) ## [1] 3.022541 Question 7 Use the CLT to approximate the probability that our estimate \\(\\bar{X}\\) is off by more than 2 grams from \\(\\mu_x\\). z_score &lt;- 2/(sqrt(sd(X)^2/12)) pnorm(-z_score) + 1- pnorm(z_score) ## [1] 0.02189533 Question 8 Now we introduce the concept of a null hypothesis. We don’t know \\(\\mu_x\\) nor \\(mu_y\\). We want to quantify what the data say about the possibility that the diet has no effect: \\(\\mu_x = \\mu_y\\). If we use CLT, then we approximate the distribution of \\(\\bar{X}\\) as normal with mean \\(mu_X\\) and standard deviation of \\(\\frac{\\sigma_X}{\\sqrt{M}}\\) and the distribution of \\(\\bar{Y}\\) and standard deviation of \\(\\frac{\\sigma_y}{\\sqrt{N}}\\), with \\(M\\) and \\(N\\) as the sample sizes for \\(X\\) and \\(Y\\) respectively, in this case 12. This implies that the difference \\(\\bar{Y} - \\bar{X}\\) has mean \\(0\\). We described that the standard deviation of this statistic (the standard error) is \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\) and that we estimate the population standard deviations \\(\\sigma_x\\) and \\(\\sigma_y\\) with the sample estimates. What is the estimate of \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\)? sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 1.469867 Question 9 So now we can compute \\(\\bar{Y}-\\bar{X}\\) as well as an estimate of this standard error and construct a t-statistic. What is this t-statistic? (mean(Y) - mean(X))/ sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 2.055174 This is a good formula to memorize because it will return in later chapters. Knowing this formula can enable us to compute p-values from scratch. Question 10 If we apply the CLT, what is the distribution of this t-statistic? The answer is A: Normal with mean 0 and standard deviation 1. T-statistic is z-score, so the values are standardized to the sample mean. Therefore, the mean is 0. Question 11 Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed for t-statistic in Question 9, when the null distribution is true? tstat &lt;- (mean(Y) - mean(X))/ sqrt((sd(X)^2 + sd(Y)^2)/12) # from question 9 2*(1-pnorm(tstat)) ## [1] 0.0398622 Question 12 CLT provides an approximation for cases in which the sample size is large. In practice, we can’t check the assumption because we only get to see 1 outcome (which you computed above). As a result, if this approximation is off, so is our p-value. As described earlier, there is another approach that does not require a large sample size, but rather that the distribution of the population is approximately normal. We don’t get to see this distribution so it is again an assumption, although we can look at the distribution of the sample with qqnorm(X) and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows t- distribution. What is the p-value under the t-distribution approximation? Hint: use the t.test function. mypar(1,2) qqnorm(X, main = &#39;Theoretical normal quantiles vs X&#39;) qqline(X) qqnorm(Y, main = &#39;Theoretical normal quantiles vs Y&#39;) qqline(Y) t.test(X,Y)$p.value ## [1] 0.05299888 Question 13 With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. They can’t both be right. What best describes the difference? The answer is B: These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation of the standard error and thus, under the null, large values are more probable under the null distribution. 2.18 Exercises For these exercises we will load the babies dataset from babies.txt. We will use this data to review the concepts behind the p-values and then test confidence interval concepts. babies &lt;- read.table(&quot;babies.txt&quot;, header=TRUE) This is a large dataset (1,236 cases), and we will pretend that it contains the entire population in which we are interested. We will study the differences in birth weight between babies born to smoking and non-smoking mothers. First, let’s split this into two birth weight datasets: one of birth weights to non-smoking mothers and the other of birth weights to smoking mothers. bwt.nonsmoke &lt;- filter(babies, smoke==0) %&gt;% dplyr::select(bwt) %&gt;% unlist bwt.smoke &lt;- filter(babies, smoke==1) %&gt;% dplyr::select(bwt) %&gt;% unlist Now, we can look for the true population difference in means between smoking and non-smoking birth weights. library(rafalib) mean(bwt.nonsmoke)-mean(bwt.smoke) ## [1] 8.937666 popsd(bwt.nonsmoke) ## [1] 17.38696 popsd(bwt.smoke) ## [1] 18.08024 The population difference of mean birth weights is about 8.9 ounces. The standard deviations of the nonsmoking and smoking groups are about 17.4 and 18.1 ounces, respectively. As we did with the mouse weight data, this assessment interactively reviews inference concepts using simulations in R. We will treat the babies dataset as the full population and draw samples from it to simulate individual experiments. We will then ask whether somebody who only received the random samples would be able to draw correct conclusions about the population. We are interested in testing whether the birth weights of babies born to non-smoking mothers are significantly different from the birth weights of babies born to smoking mothers. Question 1 Set the seed at 1 and obtain two samples, each of size N = 25, from non-smoking mothers (dat.ns) and smoking mothers (dat.s). Compute the t-statistic (call it tval). N &lt;- 25 set.seed(1) dat.ns &lt;- sample(bwt.nonsmoke,N) dat.s &lt;- sample(bwt.smoke,N) tval &lt;- (mean(dat.s) - mean(dat.ns)) / sqrt(var(dat.s)/N + var(dat.ns)/N) t.test(dat.s,dat.ns)$statistic[[1]] ## [1] -2.120904 pt(tval, df = N*2-2)*2 ## [1] 0.03912225 Question 2 Recall that we summarize our data using a t-statistics because we know that in situations where the null hypothesis is true (what we mean when we say “under the null”) and the sample size is relatively large, this t-value will have an approximate standard normal distribution. Because we know the distribution of the t-value under the null, we can quantitatively determine how unusual the observed t-value would be if the null hypothesis were true. The standard procedure is to examine the probability a t-statistic that actually does follow the null hypothesis would have larger absolute value than the absolute value of the t-value we just observed- this is called a two-sided test. We have computed these by taking one minus the area under the standard normal curve between -abs(tval) and abs(tval). In R, we can do this by using the pnorm function, which computes the area under a normal curve from negative infinity up to the value given as its first argument. What is the estimated p-value? (pnorm(tval) + 1 - pnorm(-tval)) ## [1] 0.03392985 Question 3 Because of the symmetry of the standard normal distribution, there is a simpler way to calculate the probability that a t-value under the null could have a larger absolute value than tval. Choose a simplified calculation from the four choices. 2*pnorm(-abs(tval)) ## [1] 0.03392985 Question 4 By reporting only p-values, many scientific publications provide an incomplete story of their findings. As we have mentioned, with very large sample sizes, scientifically insignificant differences between two groups can lead to small p-values. Confidence intervals are more informative as they include the estimate itself. Our estimate of the difference between babies of smokers and non-smokers: mean(dat.s) - mean( dat.ns). If we use the CLT, what quantity would we add and subtract to this estimate to obtain a 99% confidence interval? mean(dat.s) - mean(dat.ns) ## [1] -9.92 Q &lt;- qnorm(0.5 + 0.99/2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Q*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Q*se) ## [1] -21.967797 2.127797 print(Q*se) ## [1] 12.0478 Question 5 If instead of CLT, we use the t-distribution approximation, what do we add and subtract (use 2*N-2 degrees of freedom)? Qt &lt;- qt(0.5 + 0.99/2, df = N*2-2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Qt*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Qt*se) ## [1] -22.465339 2.625339 t.test(dat.s, dat.ns) ## ## Welch Two Sample t-test ## ## data: dat.s and dat.ns ## t = -2.1209, df = 47.693, p-value = 0.03916 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.3258047 -0.5141953 ## sample estimates: ## mean of x mean of y ## 114.76 124.68 print(Qt*se) ## [1] 12.54534 Question 6 Why are the values from 4 and 5 so similar? The answer is C: N and thus the degrees of freedom is large enough to make the normal and t-distributions very similar. Question 7 Which of the following sentences about a Type I error is false? The answer is C: From the original data alone, you can tell whether you have made a Type I error. Question 8 Set the seed at 1 and take a random sample of \\(N = 5\\) measurements from each of the smoking and nonsmoking datasets. What is the p-value (use the t-test function)? N &lt;- 5 set.seed(1) ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) t.test(ns_sample, s_sample)$p.value ## [1] 0.1366428 Question 9 The p-value is larger than 0.05 so using the typical cut-off, we would not reject. This is a type II error (false negative). Which of the following is not a way to decrease this type of error? The answer is C: Find a population for which the null is not true. Question 10 Set the seed at 1, then use the replicate function to repeat the code in Exercise 9 10,000 times. What proportion of the time do we reject at the 0.05 level? B &lt;- 10000 alpha &lt;- 0.05 set.seed(1) N &lt;- 5 res_list&lt;- replicate(B, { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(alpha &gt; pval) # this returns logical (i.e., TRUE if pval is smaller than 0.05 = alpha), and stores in the vector res_list. }) mean(res_list) ## [1] 0.0984 Question 11 Note that, not surprisingly, the power is lower than 10%. Repeat the exercise above for sample sizes of 30, 60, 90 and 120. Which of these four gives you power of about 80%? pval_calc &lt;- function(N) { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(pval) } Ns &lt;- c(30,60,90,120) B &lt;- 10000 alpha &lt;- 0.05 res_list &lt;- vector(&#39;double&#39;,length(Ns)) for (i in seq_along(Ns)) { res_list[[i]] &lt;- mean(replicate(B, pval_calc(Ns[[i]])) &lt; alpha) } names(res_list) &lt;- Ns print(res_list) # sample size of 60 gives power of 80% ## 30 60 90 120 ## 0.4933 0.7868 0.9344 0.9835 Sample size of 60 gives power of 80%. Question 12 Repeat Question 11, but now require an \\(\\alpha\\) level of 0.01. Which of these four gives you power of 80%? pval_calc &lt;- function(N) { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(pval) } Ns &lt;- c(30,60,90,120) B &lt;- 10000 alpha &lt;- 0.01 res_list &lt;- vector(&#39;double&#39;,length(Ns)) for (i in seq_along(Ns)) { res_list[[i]] &lt;- mean(replicate(B, pval_calc(Ns[[i]])) &lt; alpha) } names(res_list) &lt;- Ns print(res_list) # sample size of 90 gives power of 80% ## 30 60 90 120 ## 0.2461 0.5607 0.7932 0.9241 Sample size of 90 gives power of 80%. 2.21 Exercises We have used Monte Carlo simulation throughout this chapter to demonstrate statistical concepts; namely, sampling from the population. We mostly applied this to demonstrate the statistical properties related to inference on differences in averages. Here, we will consider examples of how Monte Carlo simulations are used in practice. Question 1 Imagine you are William Sealy Gosset and have just mathematically derived the distribution of the t-statistic when the sample comes from a normal distribution. Unlike Gosset you have access to computers and can use them to check the results. Let’s start by creating an outcome. Set the seed at 1, use rnorm to generate a random sample of size 5, \\(X_1,...,.X_5\\) from a standard normal distribution, then compute the t-statistic \\(t = \\sqrt{5}\\bar{X}/s\\) with \\(s\\) the sample standard deviation. What value do you observe? set.seed(1) n &lt;- 5 sample1 &lt;- rnorm(n) tstat &lt;- sqrt(5)*mean(sample1)/sd(sample1) tstat ## [1] 0.3007746 Question 2 You have just performed a Monte Carlo simulation using rnorm, a random number generator for normally distributed data. Gosset’s mathematical calculation tells us that the t-statistic defined in the previous exercise, a random variable, follows a t-distribution with \\(N-1\\) degrees of freedom. Monte Carlo simulations can be used to check the theory: we generate many outcomes and compare them to the theoretical result. Set the seed to 1, generate B = 1000 t-statistics as done in Question 1. What proportion is larger than 2? set.seed(1) samp &lt;- rnorm(5) tstat &lt;- sqrt(5)*samp/sd(samp) get_t &lt;- function(n) { samp &lt;- rnorm(n) tstat &lt;- sqrt(n)*mean(samp)/sd(samp) return(tstat) } set.seed(1) res_list &lt;- replicate(1000, get_t(5)) mean(res_list &gt; 2) ## [1] 0.068 Question 3 The answer to Question 2 is very similar to the theoretical prediction: 1-pt(2,df=4). We can check several such quantiles using qqplot function. To obtain quantiles for the t-distribution we can generate percentiles from just above 0 to just below 1: B=100; ps = seq(1/(B+1), 1-1/(B+1), len = B) and compute the quantiles with qt(ps, df=4). Now we can use qqplot to compare these theoretical quantiles to those obtained in Monte Carlo simulation. Use Monte Carlo simulation developed for Question 2 to corroborate that the t-statistic \\(t = \\sqrt{N}\\bar{X}/s\\) follows a t-distribution for several values of N. For which sample sizes does the approximation best work? B &lt;- 1000 ps = seq(1/(B+1), 1-1/(B+1),len=B) get_t &lt;- function(n) { samp &lt;- rnorm(n) tstat &lt;- sqrt(n)*mean(samp)/sd(samp) return(tstat) } Ns &lt;- c(5,10,50,100) mypar(2,2) set.seed(1) for (i in seq_along(Ns)) { res_list &lt;- replicate(1000, get_t(Ns[i])) theory_t &lt;- qt(ps,df=Ns[i]-1) qqplot(theory_t, res_list, main = paste0(&#39;sample size= &#39;,Ns[i]), xlab = &#39;theory&#39;, ylab = &#39;sim&#39;) abline(0,1) } The approximations are spot on for all sample sizes (answer choice C). Question 4 Use Monte Carlo simulation to corroborate that the t-statistic comparing two means and obtained with normally distributed (mean 0 and sd) data follows a t-distribution. In this case we will use the t.test function with var.equal=TRUE. With this argument the degrees of freedom will be df=2*N-2 with N the sample size. For which sample sizes does the approximation best work? ttestgenerator &lt;- function(n) { cases &lt;- rnorm(n) controls &lt;- rnorm(n) tstat &lt;- t.test(cases,controls)$statistic[[1]] return(tstat) } Ns &lt;- c(5,10,50,100) mypar(2,2) set.seed(1) for (i in seq_along(Ns)) { res_list &lt;- replicate(1000, ttestgenerator(Ns[i])) theory_t &lt;- qt(ps,df=2*Ns[i]-2) qqplot(theory_t, res_list, main = paste0(&#39;sample size= &#39;,Ns[i]), xlab = &#39;theory&#39;, ylab = &#39;sim&#39;) abline(0,1) } The approximations are spot on for all sample sizes (answer choice C). Question 5 Is the following statement true or false? If instead of generating the sample with X = rnorm(15), we generate it with binary data (either positive or negative 1 with probability 0.5) X = sample(c(-1,1),15,replace = TRUE) then the t-statistic tstat &lt;- sqrt(15) * mean(X) / sd(X) is approximated by a t-distribution with 14 degrees of freedom. set.seed(1) res_list &lt;- replicate(1000, { X &lt;- sample(c(-1,1),15,replace = T) tstat &lt;- sqrt(15) * mean(X)/sd(X) return(tstat) }) ps &lt;- seq(1/(B+1), 1-1/(B+1),len=B) theory_t &lt;- qt(ps,df=14) qqplot(res_list, theory_t) False. Instead, it is approximated by binomial distribution because the data are binary (only two values). Question 6 Is the following statement true or false? If instead of generating the sample with X = rnorm(N) with N = 1000, we generate it with binary data X = sample(c(-1,1),15,replace = TRUE) then the t-statistic tstat &lt;- sqrt(15) * mean(X) / sd(X) is approximated by a t-distribution with 999 degrees of freedom. set.seed(1) res_list &lt;- replicate(1000, { X &lt;- sample(c(-1,1),1000,replace = T) tstat &lt;- sqrt(1000) * mean(X)/sd(X) return(tstat) }) B&lt;-1000 ps &lt;- seq(1/(B+1), 1-1/(B+1),len=B) theory_t &lt;- qt(ps,df=999) qqplot(res_list, theory_t) abline(0,1) True. Question 7 We can derive approximation of the distribution of the sample average or the t-statistic theoretically. However, suppose we are interested in the distribution of a statistic for which a theoretical approximation is not immediately obvious. Consider the sample median as an example. Use a Monte Carlo to determine which of the following best approximates the median of a sample taken from normally distributed population with mean 0 and standard deviation 1. The answer is A: Just like for the average, the sample median is approximately normal with mean 0 and SD \\(1/\\sqrt{N}\\). 2.23 Exercises babies &lt;- read.table(&quot;babies.txt&quot;, header=TRUE) bwt.nonsmoke &lt;- filter(babies, smoke==0) %&gt;% dplyr::select(bwt) %&gt;% unlist bwt.smoke &lt;- filter(babies, smoke==1) %&gt;% dplyr::select(bwt) %&gt;% unlist Question 1 We will generate the following random variable based on a sample size of 10 and observe the following difference: N=10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke , N) smokers &lt;- sample(bwt.smoke , N) obs &lt;- mean(smokers) - mean(nonsmokers) The question is whether this observed difference is statistically significant. We do not want to rely on the assumptions needed for the normal or t-distribution approximations to hold, so instead we will use permutations. We will reshuffle the data and recompute the mean. We can create one permuted sample with the following code: dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample( dat ) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] mean(smokersstar)-mean(nonsmokersstar) ## [1] -8.5 The last value is one observation from the null distribution we will construct. Set the seed at 1, and then repeat the permutation 1,000 times to create a null distribution. What is the permutation derived p-value for our observation? N &lt;- 10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke,N) smokers &lt;- sample(bwt.smoke,N) obs &lt;- mean(smokers) - mean(nonsmokers) set.seed(1) res_list &lt;- replicate(1000, { dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample(dat) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] avgdiff &lt;- mean(smokersstar) - mean(nonsmokersstar) return(avgdiff) }) (sum(abs(res_list) &gt; abs(obs)) +1)/(length(res_list)+1) ## [1] 0.05294705 Due to the random numbers, the actual answer might differ. Question 2 Repeat the above exercise, but instead of the differences in mean, consider the differences in median obs &lt;- median(smokers) - median(nonsmokers). What is the permutation based p-value? N &lt;- 10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke,N) smokers &lt;- sample(bwt.smoke,N) obs &lt;- median(smokers) - median(nonsmokers) set.seed(1) res_list &lt;- replicate(1000, { dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample(dat) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] avgdiff &lt;- median(smokersstar) - median(nonsmokersstar) return(avgdiff) }) (sum(abs(res_list) &gt; abs(obs)) +1)/(length(res_list)+1) ## [1] 0.01798202 Due to the random numbers, the actual answer might differ. 2.25 Exercises d = read.csv(&#39;assoctest.csv&#39;) Question 1 This dataframe refects the allele status (either AA/Aa or aa) and the case/control status for 72 individuals. Compute the Chi-square test for the association of genotype with case/control status (using the table function and the chisq.test function). Examine the table to see if it looks enriched for association by eye. What is the X-squared statistic? tab &lt;- table(d) chisq.test(tab)$statistic ## X-squared ## 3.343653 Question 2 Compute Fisher’s exact test fisher.test for the same table. What is the p-value (two-tailed)? fisher.test(tab)$p.value ## [1] 0.05193834 "],
["exploratory-data-analysis.html", "Chapter 3 Exploratory Data Analysis 3.8 Exercises 3.11 Exercises", " Chapter 3 Exploratory Data Analysis Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the function filter() and %&gt;% library(rafalib) # important for plotting with base R 3.8 Exercises Question 1 Given the above histogram, how many people are between the ages of 35 and 45? 6 people Question 2 The InsectSprays dataset is included in R. The dataset reports the counts of insects in agricultural experimental units treated with different insecticides. Make a boxplot and determine which insecticide appears to be most effective (has the lowest median). dat &lt;- split(InsectSprays$count, InsectSprays$spray) boxplot(dat) C has the lowest median and is, therefore, most effective. Question 3 Download the data and load them by typing load(skew.RData) into R. Use exploratory data analysis tools to determine which two columns are different from the rest. Which column has positive skew (a long tail to the right)? load(&#39;skew.RData&#39;) boxplot(dat) hist(dat[,4]) # Column 4 has a positive skew. Notice that boxplot function automatically splits each column of the dat (total 9 columns). Hence, there are 9 boxplots total, each of which has 1000 points; the dimension of dat is 1000 x 9 (dim(dat)). Negative skew refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. Question 4 Which column has negative skew (a long tail to the left)? hist(dat[,9]) # Column 9 has a negative skew. Question 5 Let’s consider a random sample of finishers from the New York City Marathon in 2002. This dataset can be found in the UsingR package. Load the library and then load the nym.2002 dataset. library(dplyr) data(nym.2002, package=&quot;UsingR&quot;) Use boxplots and histograms to compare the finishing times of males and females. Which of the following best describes the difference? data(nym.2002, package=&quot;UsingR&quot;) male &lt;- nym.2002 %&gt;% filter(gender == &#39;Male&#39;) female &lt;- nym.2002 %&gt;% filter(gender == &#39;Female&#39;) mypar(1,2) hist(female$time, xlim = c(100,600)) hist(male$time, xlim = c(100,600)) Both histograms have a similar distribution (skewed to the right). But the center of the histogram seems to be different. We can check this by calculating the absolute difference of the mean and median. abs(mean(male$time) - mean(female$time)) ## [1] 23.11574 abs(median(male$time) - median(female$time)) ## [1] 21.70833 There is a difference of around 21-23 minutes between males and females. So answer C seems to be appropriate: Males and females have similar right skewed distributions, with the former 20 minutes shifted to the left. Question 6 Use dplyr to create two new data frames: males and females, with the data for each gender. For males, what is the Pearson correlation between age and time to finish? plot(male$age, male$time, main = &#39;male&#39;) cor(male$age, male$time) ## [1] 0.2432273 Question 7 For females, what is the Pearson correlation between age and time to finish? plot(female$age, female$time, main = &#39;female&#39;) cor(female$age, female$time) ## [1] 0.2443156 Question 8 If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender. Look at scatterplots and boxplots of times stratified by age groups (20-24, 25-30, etc.). After examining the data, what is a more reasonable conclusion? groups_m &lt;- split(male$time, floor(male$age/5)*5) # 10-14, 15-19, etc groups_f &lt;- split(female$time, floor(female$age/5)*5) # 10-14, 15-19, etc mypar(1,2) boxplot(groups_m) boxplot(groups_f) This is a tricky question because the question asks you to stratify the data into groups. Stratification can be achieved via split function. To have each group a range of 5 (ex. 25-30), all the age numbers will have to be rounded up or down so that the resulting numbers will be divisible by 5. I rounded the numbers down by using the floor function. As a result, 40 represents the 40-44 age group. You can also use the ceiling function to stratify the data, which will then be rounded up. So, 45 represents 41-45 age group. In the example below, age of 42 is categorized using both floor and ceiling functions. floor(42/5)*5 ## [1] 40 ceiling(42/5)*5 ## [1] 45 The appropriate answer is A: Finish times are constant up until about our 40s, then we get slower. Question 9 When is it appropriate to use pie charts or donut charts? Never (answer choice D) Question 10 The use of pseudo-3D plots in the literature mostly adds: Confusion (answer choice D) 3.11 Exercises First load the data. data(ChickWeight) mypar() plot(ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet) chick = reshape(ChickWeight, idvar=c(&quot;Chick&quot;,&quot;Diet&quot;), timevar=&quot;Time&quot;, direction=&quot;wide&quot;) chick = na.omit(chick) Question 1 Focus on the chick weights on day 4 (check the column names of chick and note the numbers). How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams? Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier. Hint: use c to add a number to a vector. chick_w4 &lt;- chick[,&#39;weight.4&#39;] chick_w4_add &lt;- append(chick_w4, 3000) # or use function c # chick_w4_add &lt;- c(chick_w4, 3000) chick_w4_add ## [1] 59 58 55 56 48 59 57 59 52 63 56 53 ## [13] 62 61 55 54 62 64 61 58 62 57 58 58 ## [25] 59 59 62 65 63 63 64 61 56 61 61 66 ## [37] 66 63 69 61 62 66 62 64 67 3000 mean(chick_w4_add) - mean(chick_w4) # Difference between with and without outlier ## [1] 63.90966 mean(chick_w4_add)/mean(chick_w4) # Ratio between with and without outlier ## [1] 2.062407 Question 2 In exercise 1, we saw how sensitive the mean is to outliers. Now let’s see what happens when we use the median instead of the mean. Compute the same ratio, but now using median instead of mean. Specifically, what is the median weight of the day 4 chicks, including the outlier chick, divided by the median of the weight of the day 4 chicks without the outlier. median(chick_w4_add) - median(chick_w4) # difference ## [1] 0 median(chick_w4_add)/median(chick_w4) # ratio ## [1] 1 Question 3 Now try the same thing with the sample standard deviation (the sd function in R). Add a chick with weight 3000 grams to the chick weights from day 4. How much does the standard deviation change? What’s the standard deviation with the outlier chick divided by the standard deviation without the outlier chick? sd(chick_w4_add) - sd(chick_w4) # difference ## [1] 429.1973 sd(chick_w4_add)/ sd(chick_w4) # ratio ## [1] 101.2859 Question 4 Compare the result above to the median absolute deviation in R, which is calculated with the mad function. Note that the mad is unaffected by the addition of a single outlier. The mad function in R includes the scaling factor 1.4826, such that mad and sd are very similar for a sample from a normal distribution. What’s the MAD with the outlier chick divided by the MAD without the outlier chick? mad(chick_w4_add) - mad(chick_w4) # difference ## [1] 0 mad(chick_w4_add)/ mad(chick_w4) # ratio ## [1] 1 Question 5 Our last question relates to how the Pearson correlation is affected by an outlier as compared to the Spearman correlation. The Pearson correlation between x and y is given in R by cor(x,y). The Spearman correlation is given by cor(x,y,method=\"spearman\"). Plot the weights of chicks from day 4 and day 21. We can see that there is some general trend, with the lower weight chicks on day 4 having low weight again on day 21, and likewise for the high weight chicks. Calculate the Pearson correlation of the weights of chicks from day 4 and day 21. Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers. chick_w21 &lt;- chick[, &#39;weight.21&#39;] chick_w21 ## [1] 205 215 202 157 223 157 305 98 124 175 205 96 266 142 157 ## [16] 117 331 167 175 74 265 251 192 233 309 150 256 305 147 341 ## [31] 373 220 178 290 272 321 204 281 200 196 238 205 322 237 264 plot(chick_w4, chick_w21) cor(chick_w4,chick_w21) # correlation before ## [1] 0.4159499 chick_w21_add &lt;- append(chick_w21, 3000) cor(chick_w4_add, chick_w21_add) # correlation after outlier ## [1] 0.9861002 cor(chick_w4_add, chick_w21_add)/cor(chick_w4,chick_w21) # ratio between after and before ## [1] 2.370719 Question 6 Save the weights of the chicks on day 4 from diet 1 as a vector x. Save the weights of the chicks on day 4 from diet 4 as a vector y. Perform a t-test comparing x and y(in R the function t.test(x,y) will perform the test). Then perform a Wilcoxon test of x and y (in R the function wilcox.test(x,y) will perform the test). A warning will appear that an exact p-value cannot be calculated with ties, so an approximation is used, which is fine for our purposes. Perform a t-test of x and y, after adding a single chick of weight 200 grams to x (the diet 1 chicks). What is the p-value from this test? The p-value of a test is available with the following code: t.test(x,y)$p.value x &lt;- chick %&gt;% filter(Diet == 1) x &lt;- x[,&#39;weight.4&#39;] y &lt;- chick %&gt;% filter(Diet == 4) y &lt;- y[,&#39;weight.4&#39;] t.test(x,y)$p.value # t.test result with no outlier ## [1] 7.320259e-06 wilcox.test(x,y)$p.value # wilcox result with no outlier ## Warning in wilcox.test.default(x, y): cannot compute exact p- ## value with ties ## [1] 0.0002011939 x_add &lt;- c(x,200) # outlier added t.test(x_add,y)$p.value # t-test after outlier ## [1] 0.9380347 Question 7 Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has fewer assumptions than the t-test on the distribution of the underlying data. wilcox.test(x_add,y)$p.value # even with outlier, p-value is not perturbed ## Warning in wilcox.test.default(x_add, y): cannot compute exact p- ## value with ties ## [1] 0.0009840921 Question 8 We will now investigate a possible downside to the Wilcoxon-Mann-Whitney test statistic. Using the following code to make three boxplots, showing the true Diet 1 vs 4 weights, and then two altered versions: one with an additional difference of 10 grams and one with an additional difference of 100 grams. Use the x and y as defined above, NOT the ones with the added outlier. library(rafalib) mypar(1,3) boxplot(x,y) boxplot(x,y+10) boxplot(x,y+100) What is the difference in t-test statistic (obtained by t.test(x,y)$statistic) between adding 10 and adding 100 to all the values in the group y? Take the the t-test statistic with x and y+10 and subtract the t-test statistic with x and y+100. The value should be positive. t.test(x,y+10)$statistic - t.test(x,y+100)$statistic ## t ## 67.75097 Question 9 Examine the Wilcoxon test statistic for x and y+10 and for x and y+100. Because the Wilcoxon works on ranks, once the two groups show complete separation, that is, all points from group y are above all points from group x, the statistic will not change, regardless of how large the difference grows. Likewise, the p-value has a minimum value, regardless of how far apart the groups are. This means that the Wilcoxon test can be considered less powerful than the t-test in certain contexts. In fact, for small sample sizes, the p-value can’t be very small, even when the difference is very large. What is the p-value if we compare c(1,2,3) to c(4,5,6) using a Wilcoxon test? wilcox.test(x,y+10)$p.value ## Warning in wilcox.test.default(x, y + 10): cannot compute exact ## p-value with ties ## [1] 5.032073e-05 wilcox.test(x,y+100)$p.value ## Warning in wilcox.test.default(x, y + 100): cannot compute exact ## p-value with ties ## [1] 5.032073e-05 wilcox.test(c(1,2,3),c(4,5,6))$p.value # Answer ## [1] 0.1 Question 10 What is the p-value if we compare c(1,2,3) to c(400,500,600) using a Wilcoxon test? wilcox.test(c(1,2,3),c(400,500,600))$p.value ## [1] 0.1 "],
["matrix-algebra.html", "Chapter 4 Matrix Algebra 4.2 Exercises 4.6 Exercises 4.8 Exercises 4.10 Exercises", " Chapter 4 Matrix Algebra Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the function filter() and %&gt;% library(rafalib) # important for plotting with base R 4.2 Exercises #install.packages(&#39;UsingR&#39;) data(&#39;father.son&#39;, package = &#39;UsingR&#39;) Question 1 What is the average height of the sons (don’t round off)? y &lt;- father.son$sheight # son x &lt;- father.son$fheight # father mean(y) ## [1] 68.68407 Question 2 One of the defining features of regression is that we stratify one variable based on others. In statistics, we use the verb ‘condition.’ For exmaple,, the linear model for son and father heights answers the question: how tall do I expect a son to be if I condition on his father being \\(x\\) inches? The regression line answers this question for any \\(x\\). Using the father.son dataset described above, we want to know the expected height of sons, if we condition on the father being 71 inches. Create a list of son heights for sons that have fathers with heights of 71 inches, rounding to the nearest inch. What is the mean of the son heights for fathers that have a height of 71 inches (don’t round off your answer)? Hint: use the function round on the fathers’ heights. groups &lt;- split(y, round(x)) mean(groups[&#39;71&#39;] %&gt;% unlist()) ## [1] 70.54082 Question 3 of the following cannot be written as a linear model? The answer is C: \\(Y = a + b^t + \\epsilon\\). This is because the variable \\(t\\) is an exponent unlike all the other ansewr choices. Question 4 Suppose you model the relationship between weight and height across individuals with a linear model. You assume that the height of individuals for a fixed weight \\(x\\) follows a linear model \\(Y = a + bx + \\epsilon\\). Which of the following do you feel best describes what \\(\\epsilon\\) represents? The answer is D: Between individual variability: people of the same height vary in their weight. Let’s look at each answer choice. Let’s first think \\(Y\\) as son’s height and \\(x\\) as father’s height. So in this linear model, our goal is to predict son’s height based on father’s height. Choice A (Measurement error: scales are not perfect) seems not wrong. However, if the father’s height is 71 inches, can we gurantee that the son’s height is a certain number with a small measurement variability? This is not true because the son’s range of height can still be wide even if the father’s height is 71 inches. In fact, siblings can have very different heights even if their biological parents are identical. Since choice A only describes the \\(\\epsilon\\) as measurement variability, the description seems inadequate. This explanation also applies to Choice B (Within individual random fluctuations: you don’t weigh the same in the morning as in the afternoon) and C (Round off error introduced by the computer). Therefore, choice D seems to be most appropriate. 4.6 Exercises Question 1 In R we have vectors and matrices. You can create your own vectors with the function c. c(1,5,3,4) ## [1] 1 5 3 4 They are also the output of many functions such as: rnorm(10) ## [1] -0.8043316 -1.0565257 -1.0353958 -1.1855604 -0.5004395 ## [6] -0.5249887 -0.3024330 0.4719681 -0.2483839 1.2593180 You can turn vectors into matrcies using functions such as rbind, cbind, or matrix. Create the matrix from the vector 1:1000 like this: X = matrix(1:1000,100,10) What is the entry in row 25, column 3? X[25,3] ## [1] 225 Question 2 Using the function cbind, create a 10 x 5 matrix with first column x=1:10. Then add 2*x, 3*x, 4*x and 5*x to columns 2 through 5. What is the sum of the elements of the 7th row? first_column &lt;- 1:10 y &lt;- cbind(x1=first_column, x2=first_column*2, x3=first_column*3, x4=first_column*4, x5=first_column*5) sum(y[7,]) ## [1] 105 Question 3 Which of the following creates a matrix with multiples of 3 in the third column? matrix(1:60,20,3) # choice A ## [,1] [,2] [,3] ## [1,] 1 21 41 ## [2,] 2 22 42 ## [3,] 3 23 43 ## [4,] 4 24 44 ## [5,] 5 25 45 ## [6,] 6 26 46 ## [7,] 7 27 47 ## [8,] 8 28 48 ## [9,] 9 29 49 ## [10,] 10 30 50 ## [11,] 11 31 51 ## [12,] 12 32 52 ## [13,] 13 33 53 ## [14,] 14 34 54 ## [15,] 15 35 55 ## [16,] 16 36 56 ## [17,] 17 37 57 ## [18,] 18 38 58 ## [19,] 19 39 59 ## [20,] 20 40 60 matrix(1:60,20,3, byrow=T) # choice B ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 ## [5,] 13 14 15 ## [6,] 16 17 18 ## [7,] 19 20 21 ## [8,] 22 23 24 ## [9,] 25 26 27 ## [10,] 28 29 30 ## [11,] 31 32 33 ## [12,] 34 35 36 ## [13,] 37 38 39 ## [14,] 40 41 42 ## [15,] 43 44 45 ## [16,] 46 47 48 ## [17,] 49 50 51 ## [18,] 52 53 54 ## [19,] 55 56 57 ## [20,] 58 59 60 x = 11:20; rbind(x,2*x,3*x) # choice C ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## x 11 12 13 14 15 16 17 18 19 20 ## 22 24 26 28 30 32 34 36 38 40 ## 33 36 39 42 45 48 51 54 57 60 x = 1:40; matrix(3*x,20,2) # choice D ## [,1] [,2] ## [1,] 3 63 ## [2,] 6 66 ## [3,] 9 69 ## [4,] 12 72 ## [5,] 15 75 ## [6,] 18 78 ## [7,] 21 81 ## [8,] 24 84 ## [9,] 27 87 ## [10,] 30 90 ## [11,] 33 93 ## [12,] 36 96 ## [13,] 39 99 ## [14,] 42 102 ## [15,] 45 105 ## [16,] 48 108 ## [17,] 51 111 ## [18,] 54 114 ## [19,] 57 117 ## [20,] 60 120 The answer is B. 4.8 Exercises Question 1 Suppose \\(X\\) is a matrix in R. Which of the following is not equivalent to \\(X\\)? head(t(t(X))) # A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 head(X %*% matrix(1,ncol(X))) # B ## [,1] ## [1,] 4510 ## [2,] 4520 ## [3,] 4530 ## [4,] 4540 ## [5,] 4550 ## [6,] 4560 head(X*1) # C ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 head(X %*% diag(ncol(X))) # D ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 The answer is B. In choice A, the trasnposed matrix \\(X\\) gets transposed again, thereby returning to its original matrix \\(X\\). In choice B, the matrix \\(X\\) gets multiplied by 1, which is a scalar. So it will not be changed. In choice D, \\(X\\) gets multiplied by an identity matrix, so \\(X\\) does not change even after the matrix multiplication. Therefore, the answer is B. Question 2 Solve the following system of equations using R: \\[ \\begin{align*} 3a + 4b - 5c + d=10\\\\ 2a + 2b + 2c - d = 5\\\\ a - b + 5c - 5d = 7 \\\\ 5a + 5d = 4 \\end{align*} \\] What is the solution for \\(c\\)? X &lt;- matrix(c(3,4,-5,1,2,2,2,-1,1,-1,5,-5,5,0,0,1),4,4,byrow=T) ans &lt;- solve(X) %*% matrix(c(10,5,7,4),4,1) ans[3] ## [1] -0.8849558 Question 3 Load the following two matrices into R: a &lt;- matrix(1:12, nrow=4) b &lt;- matrix(1:15, nrow=3) What is the value in the 3rd row and the 2nd column of the matrix product of a and b? c &lt;- a %*% b c[3,2] ## [1] 113 Question 4 Multiply the 3rd row of a with the 2nd column of b, using the element-wise vector multiplication with *. What is the sum of the elements in the resulting vector? sum(a[3,] * b[,2]) ## [1] 113 4.10 Exercises Question 1 Suppose we are analyzing a set of 4 samples. The first two samples are from a treatment group A and the second two samples are from a treatment group B. This design can be represented with a model matrix like so: X &lt;- matrix(c(1,1,1,1,0,0,1,1),nrow=4) rownames(X) &lt;- c(&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;) X ## [,1] [,2] ## a 1 0 ## a 1 0 ## b 1 1 ## b 1 1 Suppose that the fitted parameters for a linear model give us: beta &lt;- c(5,2) Use the matrix multiplication operator, %*%, in R to answer the following questions: What is the fitted value for the A samples? (The fitted Y values.) X &lt;- matrix(c(1,1,1,1,0,0,1,1),nrow=4) rownames(X) &lt;- c(&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;) #beta &lt;- c(5,2) beta &lt;- matrix(c(5,2),nrow =2, ncol=1) # matrix form of the vector beta X[1:2,] %*% beta ## [,1] ## a 5 ## a 5 When I perform matrix multiplication in R %*%, I usually make sure that all my vectors are converted into matrix. In this case, I rewrote the beta variable in the function of matrix. However, this is not necessary. The vector form of beta beta &lt;- c(5,2) works well too. Question 2 What is the fitted value for the B samples? (The fitted Y values.) X[3:4,] %*% beta ## [,1] ## b 7 ## b 7 Question 3 Suppose now we are comparing two treatments B and C to a control group A, each with two samples. This design can be represented with a model matrix like so: X &lt;- matrix(c(1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1),nrow=6) rownames(X) &lt;- c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;) X ## [,1] [,2] [,3] ## a 1 0 0 ## a 1 0 0 ## b 1 1 0 ## b 1 1 0 ## c 1 0 1 ## c 1 0 1 Suppose that the fitted values for the linear model are given by: beta &lt;- c(10,3,-3) What is the fitted values for the B sample? beta &lt;- matrix(c(10,3,-3),nrow = 3) X[3:4,] %*% beta ## [,1] ## b 13 ## b 13 Question 4 What is the fitted values for the C sample? X[5:6,] %*% beta ## [,1] ## c 7 ## c 7 "],
["linear-models.html", "Chapter 5 Linear Models 5.1 Exercises 5.3 Exercises 5.5 Exercises 5.7 Exercises 5.11 Exercises 5.15 Exercises", " Chapter 5 Linear Models Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the function filter() and %&gt;% library(rafalib) # important for plotting with base R library(contrast) library(Matrix) 5.1 Exercises Question 1 We have shown how to find the least squares estimates with matrix algebra. These estimates are random variables as they are linear combinations of the data. For these estimates to be useful, we also need to compute the standard errors. Here we review standard errors in the context of linear models. To see this, we can run a Monte Carlo simulation to imitate the collection of falling object data. Specifically, we will generate the data repeatedly and compute the estimate for the quadratic term each time. g = 9.8 h0 = 56.67 v0 = 0 n = 25 tt = seq(0,3.4,len=n) y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1) Now we act as if we didn’t know \\(h0\\), \\(v0\\) and \\(-0.5*g\\) and use regression to estimate these. We can rewrite the model as \\(y = \\beta_0 + \\beta_1t + \\beta_2t^2 + \\epsilon\\) and obtain the LSE we have used in this class. Note that \\(g = -2\\beta_2\\). To obtain the LSE in R we could write: X = cbind(1,tt,tt^2) A = solve(crossprod(X))%*%t(X) Given how we have defined A, which of the following is the LSE of \\(g\\), the accerleration due to gravity? Y = matrix(y,nrow=length(y), ncol = 1) # convert vector y into matrix Y betahat &lt;- solve(crossprod(X)) %*% t(X) %*%Y betahat &lt;- A %*% Y # both ways work fine betahat[[3]] * -2 # beta2 * -2 = gravity ## [1] 9.279798 # this is equal to -2 * (A %*% Y)[3] ## [1] 9.279798 The answer is C. Answer A is wrong because 9.8 is not an estimate; it is the exact value we are looking for. Due to measurement error rnorm(n, sd=1) that is generated in y, we will never achieve 9.8. Answer B is wrong because this gives three coefficients (intercept h0, v0 and -0.5g). Answer D is wrong because A is created solely from the model matrix X, not also from the data y. Question 2 In the lines of code above, the function rnorm introduced randomness. This means that each time the lines of code above are repeated, the estimate of g will be different. Use the code above in conjunction with the function replicate to generate 100,000 Monte Carlo simulated datasets. For each dataset, compute an estimate of g. (Remember to multiply by -2.) What is the standard error of this estimate? set.seed(1) gravity_list &lt;- replicate(100000, { y = h0 + v0 *tt - 0.5* g*tt^2 + rnorm(n,sd=1) Y = matrix(y,nrow=length(y), ncol = 1) betahat &lt;- solve(crossprod(X)) %*% t(X) %*%Y third_beta &lt;- betahat[[3]] * -2 return(third_beta) }) popsd(gravity_list) ## [1] 0.4297449 Function popsd, instead of sd, is used because we are not dealing with a sample of coefficients. sd adjusts for the bias in a sample by using denominator n-1 rather than n. In this case, there is no reason to adjust for the bias because we are directly interested in measuring the spread of the distribution of third_beta. Standard error of a random variable is the standard deviation of the distribution; gravity_list captures the distribution of third_beta. Therefore we compute standard deviation. Question 3 In the father and son height examples, we have randomness because we have a random sample of father and son pairs. For the sake of illustration, let’s assume that this is the entire population: library(UsingR) x = father.son$fheight y = father.son$sheight n = length(y) Now let’s run a Monte Carlo simulation in which we take a sample of size 50 over and over again. Here is how we obtain one sample: N = 50 index = sample(n,N) sampledat = father.son[index,] x = sampledat$fheight y = sampledat$sheight betahat = lm(y~x)$coef Use the function replicate to take 10,000 samples. What is the standard error of the slope estimate? That is, calculate the standard deviation of the estimate from the observed values obtained from many random samples. x = father.son$fheight y = father.son$sheight n = length(y) N = 50 set.seed(1) output &lt;- replicate(10000, { index = sample(n,N) sampledat = father.son[index,] x = sampledat$fheight y = sampledat$sheight betahat = lm(y~x)$coef return(betahat[[2]]) }) popsd(output) ## [1] 0.1243209 Question 4 Which of the following is closest to the covariance between father heights and son heights? #x = father.son$fheight #y = father.son$sheight mean((y - mean(y))*(x-mean(x))) # closest to 4 ## [1] 3.869739 5.3 Exercises Question 1 Given the factors we have defined above and without defining any new ones, which of the following R formula will produce a design matrix (model matrix) that lets us analyze the effect of condition, controlling for the different days? day &lt;- c(&#39;A&#39;,&#39;A&#39;,&#39;B&#39;,&#39;B&#39;,&#39;C&#39;,&#39;C&#39;) condition &lt;- c(&#39;control&#39;,&#39;treatment&#39;,&#39;control&#39;,&#39;treatment&#39;,&#39;control&#39;,&#39;treatment&#39;) model.matrix(~day+condition) # answer is A ## (Intercept) dayB dayC conditiontreatment ## 1 1 0 0 0 ## 2 1 0 0 1 ## 3 1 1 0 0 ## 4 1 1 0 1 ## 5 1 0 1 0 ## 6 1 0 1 1 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$day ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$condition ## [1] &quot;contr.treatment&quot; 5.5 Exercises Question 1 You can make a design matrix X for a two group comparison, either using model.matrix or simply with: #X &lt;- cbind(rep(1,Nx + Ny),rep(c(0,1),c(Nx, Ny))) In order to compare two groups, where the first group has Nx=5 samples and the second group has Ny=7 samples, what is the element in the 1st row and 1st column of \\(X^TX\\)? Nx &lt;- 5 Ny &lt;- 7 X &lt;- cbind(rep(1,Nx+Ny), rep(c(0,1),c(Nx,Ny))) crossprod(X)[1,1] # this is equal to Nx + Ny ## [1] 12 Question 2 The other entries of \\(X^TX\\) are all the same. What is the number? crossprod(X) # 7, this is equal to Ny ## [,1] [,2] ## [1,] 12 7 ## [2,] 7 7 5.7 Exercises library(UsingR) N &lt;- 50 set.seed(1) index &lt;- sample(n,N) sampledat &lt;- father.son[index,] x &lt;- sampledat$fheight y &lt;- sampledat$sheight betahat &lt;- lm(y~x)$coef Question 1 The fitted values \\(\\hat{Y}\\) from a linear model can be obtained with: fit &lt;- lm(y~x) fit$fitted.values ## 1 2 3 4 5 6 7 ## 70.62707 70.36129 70.86093 68.73019 65.59181 70.55285 70.21256 ## 8 9 10 11 12 13 14 ## 68.62521 67.06729 69.64913 69.09958 71.70621 68.31598 70.57027 ## 15 16 17 18 19 20 21 ## 70.39537 70.39613 68.73977 68.98874 71.47021 72.03615 69.55975 ## 22 23 24 25 26 27 28 ## 68.15895 66.63557 71.53651 69.57083 69.71050 67.14263 70.99719 ## 29 30 31 32 33 34 35 ## 67.11046 69.04901 66.65243 67.82895 68.24209 70.70156 65.50431 ## 36 37 38 39 40 41 42 ## 67.36000 69.30065 67.94424 66.35150 71.40489 71.64301 66.81654 ## 43 44 45 46 47 48 49 ## 69.22900 69.11769 69.21793 69.69519 67.00674 68.67869 67.40752 ## 50 ## 69.28800 What is the sum of the squared residuals, where residuals are given by \\(r_i = Y_i - \\hat{Y}_i\\)? sum((y -fit$fitted.values)^2) ## [1] 256.2152 resid = y -fit$fitted.values # residual sum(resid^2) ## [1] 256.2152 In the four questions, we will calculate the standard error of the least square estimates (i.e., coefficients / beta hats from the linear model). This can easily be achieved with function lm(), but we will explore what happens within lm(). It is important to recall that \\(\\hat{Y}_i\\) is the fitted values from the model \\(X\\hat{\\beta}\\), and that \\(Y\\) is the actual data. A difference between these two is the residual. Question 2 Our estimate of \\(\\sigma^2\\) will be the sum of squared residuals divided by \\(N-p\\), the sample size minus the number of terms in the model. Since we have a sample of 50 and 2 terms in the model (an intercept and a slope), our estimate of \\(\\sigma^2\\) will be the sum of squared residuals divided by 48. Use the answer from Question 1 to provide an estimate of \\(\\sigma^2\\). s2 &lt;- sum(resid^2)/48 s2 ## [1] 5.337816 So what exactly is \\(\\sigma^2\\)? To understand it, we need to revisit a typical linear model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\), where _0 is the intercept. When you are estimating \\(Y_i\\) from a model, you can get variance for \\(Y_i\\). However, since the parameters (\\(\\beta_0\\) and \\(\\beta_1\\)) are assumed to be fixed (known values), the variance of the \\(Y_i\\) only comes from the \\(\\epsilon_i\\). Therefore \\(\\sigma^2\\) is basically the variance of \\(\\epsilon\\). Question 3 Form the design matrix \\(X\\) (Note: use a capital X). This can be done by combining a column of 1’s with a column containing x, the fathers’ heights. N &lt;- 50 X &lt;- cbind(rep(1,N),x) Now calculate \\((X^TX)^{-1}\\). Use the solve function for the inverse and t for the transpose. What is the element in the first row, first column? X &lt;- model.matrix(~x) solve(crossprod(X))[1,1] ## [1] 11.30275 Remember that crossprod(X) is equivalent to t(X) %*% X. Question 4 Now we are one step away from the standard error of \\(\\hat{\\beta}\\). Take the diagonals from the \\((X^TX)^{-1}\\) matrix above, using the diag function. Multiply our estmate of \\(\\sigma^2\\) and the diagonals of this matrix. This is the estimated variance of \\(\\hat{\\beta}\\), so take the square root of this. You should end up two numbers: the standard error for the intercept and the standard error for the slope. What is the standard error for the slope? sqrt(diag(solve(crossprod(X))) * s2)[2] ## x ## 0.1141966 Although the questions are relatively straightforward, the math derivation is not. Do not worry if you do not understand the derivation. The only thing that matters is that we have just computed the standard error of a coefficient in linear regression. And this can all be done with function lm. 5.11 Exercises Suppose we have an experiment with two species A and B, and two conditions, control and treated. species &lt;- factor(c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;)) condition &lt;- factor(c(&quot;control&quot;,&quot;treated&quot;,&quot;control&quot;,&quot;treated&quot;)) We will use the formula of ~ species + condition to create the model matrix: model.matrix(~species + condition) Question 1 Suppose we want to build a contrast of coefficients for the above experimental design. You can either figure this question out by looking at the design matrix, or by using the contrast function from the contrast library with random numbers for y. The contrast vector will be returned as contrast(...)$X. What should the contrast vector be, to obtain the difference between the species B control group and the species A treatment group (species B control - species A treatment)? Assume that the coeffcients (columns of design matrix) are: Intercept, speciesB, condition-treated. fit &lt;- lm(rnorm(4) ~ species + condition) table(species,condition) # just like in the species vector, there are four units total. ## condition ## species control treated ## A 1 1 ## B 1 1 etc &lt;- contrast(fit, list(species =&#39;B&#39;,condition=&#39;control&#39;), list(species =&#39;A&#39;,condition=&#39;treated&#39;)) etc$X ## (Intercept) speciesB conditiontreated ## 1 0 1 -1 ## attr(,&quot;assign&quot;) ## [1] 0 1 2 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$species ## [1] &quot;contr.treatment&quot; ## ## attr(,&quot;contrasts&quot;)$condition ## [1] &quot;contr.treatment&quot; I made a model matrix by using rnorm(4), which generates four random numbers (mean=0, sd=1). Then I performed a contrast between species B control and species A treatment groups using the contrast function. I stored the results in etc, which shows that the answer is D (0,1,-1). Question 2 Use the Rmd script to load the spider dataset. Suppose we build a model using two variables: ~ type + leg. What is the t-statistic for the contrast of leg pair L4 vs. leg pair L2? spider &lt;- read.csv(&quot;spider_wolff_gorb_2013.csv&quot;, skip=1) fit &lt;- lm(friction~type+leg, data = spider) res &lt;- contrast(fit, list(leg =&#39;L4&#39;,type = &#39;pull&#39;), list(leg = &#39;L2&#39;,type=&#39;pull&#39;)) res$testStat ## 1 ## 2.451974 Question 3 X &lt;- model.matrix(~ type + leg, data=spider) Sigma.hat &lt;- sum(fit$residuals^2)/(nrow(X) - ncol(X)) * solve(t(X) %*% X) Sigma.hat ## (Intercept) typepush legL2 ## (Intercept) 0.0007929832 -3.081306e-04 -0.0006389179 ## typepush -0.0003081306 6.162612e-04 0.0000000000 ## legL2 -0.0006389179 -6.439411e-20 0.0020871318 ## legL3 -0.0006389179 -6.439411e-20 0.0006389179 ## legL4 -0.0006389179 -1.191291e-19 0.0006389179 ## legL3 legL4 ## (Intercept) -0.0006389179 -0.0006389179 ## typepush 0.0000000000 0.0000000000 ## legL2 0.0006389179 0.0006389179 ## legL3 0.0010566719 0.0006389179 ## legL4 0.0006389179 0.0011819981 Using the estimate \\(\\Sigma\\) (estimated covariance matrix), what is your estimate of \\(cov(\\hat{\\beta}_{L4},\\hat{\\beta}_{L2})\\)? Our contrast matrix for the desired comparison is: C &lt;- matrix(c(0,0,-1,0,1),1,5) Sigma.hat &lt;- sum(fit$residuals^2)/(nrow(X)-ncol(X)) * solve(t(X)%*%X) # covariance matrix The answer is 0.0006389179. It is important to know that sqrt(diag(Sigma.hat)) gives standard error of the least square estimates. Question 4 Suppose that we notice that the within-group variances for the groups with smaller frictional coeffcients are generally smaller, and so we try to apply a transformation to the frictional coeffcients to make the within-group variances more constant. Add a new variable log2friction to the spider dataframe: spider$log2friction &lt;- log2(spider$friction) The Y values now look like: boxplot(log2friction ~ type*leg, data=spider) Run a linear model of log2friction with type, leg and interactions between type and leg. What is the t-statistic for the interaction of type push and leg L4? If this t-statistic is sufficiently large, we would reject the null hypothesis that the push vs. pull effect on log2(friction) is the same in L4 as in L1. logfit &lt;- lm(log2friction~type+leg+type:leg, data = spider) summary(logfit)[[4]] # t-statistic of typepush:legL4 = -3.689 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1682816 0.06613097 -2.5446712 1.148701e-02 ## typepush -1.2065650 0.09352331 -12.9012220 4.472641e-30 ## legL2 0.3468125 0.11952459 2.9015992 4.014075e-03 ## legL3 0.4899946 0.08504571 5.7615441 2.237221e-08 ## legL4 0.6466753 0.08994784 7.1894475 6.199475e-12 ## typepush:legL2 0.0996718 0.16903330 0.5896578 5.559060e-01 ## typepush:legL3 -0.5407473 0.12027280 -4.4960067 1.023073e-05 ## typepush:legL4 -0.4692035 0.12720545 -3.6885485 2.719589e-04 Question 5 Using the same analysis of log2 transformed data, what is the F-value for all of the type:leg interaction terms in an ANOVA? If this value is sufficiently large, we would reject the null hypothesis that the push vs. pull effect on log2(friction) is the same for all leg pairs. anova(logfit) # F-value of type:leg: 10.701 ## Analysis of Variance Table ## ## Response: log2friction ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## type 1 164.709 164.709 1107.714 &lt; 2.2e-16 *** ## leg 3 7.065 2.355 15.838 1.589e-09 *** ## type:leg 3 4.774 1.591 10.701 1.130e-06 *** ## Residuals 274 40.742 0.149 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.15 Exercises Question 1 Consider these design matrices: \\[ \\, A = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 1\\\\ \\end{pmatrix} \\ B = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 0\\\\ \\end{pmatrix} \\ C = \\begin{pmatrix} 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 2\\\\ 1 &amp; 2 &amp; 4\\\\ 1 &amp; 3 &amp; 6\\\\ \\end{pmatrix} \\] \\[ \\, D = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 1 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 0\\\\ 1 &amp; 0 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{pmatrix} \\ E = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 1 &amp; 0 &amp; 1 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 1\\\\ \\end{pmatrix} \\ F = \\begin{pmatrix} 1 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 0 &amp; 1\\\\ 1 &amp; 0 &amp; 1 &amp; 1\\\\ 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 0 &amp; 0\\\\ 1 &amp; 1 &amp; 1 &amp; 0\\\\ \\end{pmatrix} \\] Which of the above design matrices does NOT have the problem of collinearity? The answer is E. Collinearity exists when a column within the matrix is a linear combination of other columns in the same matrix. If a model matrix has collinearity, then it has a confound, thereby becoming problematic. In answer A, the second column is a linear combination of the third and fourth columns. In answer B, the first column is a linear combination of the second and fourth columns. In answer C, the third column is a multiple of the second column by a factor of 2. In answer D, the fourth column is a linear combination of the second and third columns. In answer F, the first column is a linear combination of the second and fourth columns. Question 2 sex &lt;- factor(rep(c(&quot;female&quot;,&quot;male&quot;),each=4)) trt &lt;- factor(c(&quot;A&quot;,&quot;A&quot;,&quot;B&quot;,&quot;B&quot;,&quot;C&quot;,&quot;C&quot;,&quot;D&quot;,&quot;D&quot;)) X &lt;- model.matrix( ~ sex + trt) qr(X)$rank ## [1] 4 Y &lt;- 1:8 makeYstar &lt;- function(a,b) Y - X[,2] * a - X[,5] * b fitTheRest &lt;- function(a,b) { Ystar &lt;- makeYstar(a,b) Xrest &lt;- X[,-c(2,5)] betarest &lt;- solve(t(Xrest) %*% Xrest) %*% t(Xrest) %*% Ystar residuals &lt;- Ystar - Xrest %*% betarest sum(residuals^2) } What is the sum of squared residuals when the male coefficient is 1 and D coefficient is 2, and the other coefficients are fit using the linear model solution? fitTheRest(1,2) ## [1] 11 Xrest refers to the model matrix that does not have \\(\\beta_{male}\\) and \\(\\beta_D\\). In this question, these two coefficients are fixed. Therefore, Xrest is used as a model matrix to compute least square estimates for the other three coefficients in X &lt;- model.matrix( ~ sex + trt).Try to recreate the custom function fitTheRest on your own. Question 3 We can apply our function fitTheRest to a grid of values for female and \\(\\beta_D\\), using the outer function in R. outer takes three arguments: a grid of values for the first argument, a grid of values for the second argument, and finally a function which takes two arguments. Try it out: outer(1:3,1:3,&#39;*&#39;) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 ## [3,] 3 6 9 We can run fitTheRest on a grid of values, using the following code (the Vectorize is necessary as outer requires only vectorized functions) outer(-2:8,-2:8,Vectorize(fitTheRest)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] ## [1,] 102 83 66 51 38 27 18 11 6 3 2 ## [2,] 83 66 51 38 27 18 11 6 3 2 3 ## [3,] 66 51 38 27 18 11 6 3 2 3 6 ## [4,] 51 38 27 18 11 6 3 2 3 6 11 ## [5,] 38 27 18 11 6 3 2 3 6 11 18 ## [6,] 27 18 11 6 3 2 3 6 11 18 27 ## [7,] 18 11 6 3 2 3 6 11 18 27 38 ## [8,] 11 6 3 2 3 6 11 18 27 38 51 ## [9,] 6 3 2 3 6 11 18 27 38 51 66 ## [10,] 3 2 3 6 11 18 27 38 51 66 83 ## [11,] 2 3 6 11 18 27 38 51 66 83 102 In the grid of values, what is the smallest sum of squared residuals? outer(1:3,1:3,&#39;*&#39;) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 2 4 6 ## [3,] 3 6 9 ans &lt;- outer(-2:8,-2:8,Vectorize(fitTheRest)) unique(ans[which(ans == min(ans))]) ## [1] 2 "]
]
