[
["index.html", "Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Welcome Acknowledgment Frequently Asked Questions", " Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Seung Hyun (Sam) Min 2020-12-09 Welcome This book contains unofficial exercise solutions for the book Data Analysis for the Life Sciences with R by Rafael A. Irizarry and Michael I. Love. The PDF copy of the book is available for free and the physical copy is available in Amazon. Acknowledgment I would like to thank Rafael A. Irizarry and Michael I. Love for writing this wonderful book, and my friends who encouraged me to undertake this project. Frequently Asked Questions You can read the FAQs in the github page. "],
["getting-started.html", "Chapter 1 Getting started", " Chapter 1 Getting started Since this chapter does not deal with statistics, I have decided to skip this chapter altogether. "],
["inference.html", "Chapter 2 Inference 2.1 Ch 2.9 Exercises 2.2 Ch 2.11 Exercises 2.3 Ch 2.13 Exercises 2.4 Ch 2.18 Exercises", " Chapter 2 Inference First, upload necessary package(s). library(tidyverse) #also uploads dplyr library(rafalib) # important for plotting with base R Note: bolded parts in the question denote modified wordings for clarity. ## Ch 2.7 Exercises If you have not downloaded the data before, dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleControlsPopulation.csv&quot; url &lt;- paste0(dir, filename) x &lt;- unlist(read.csv(url)) Or if you already have downloaded the data, then just upload it. dat &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) bodyweight &lt;- select(dat, Bodyweight) x &lt;- unlist(bodyweight) # or use pipe %&gt;% x &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() Check out what unlist does by typing ?unlist in the command. The second method is more concise because of the pipe %&gt;%, which allows multiple lines of commands to be in one continuous line. 2.0.1 Question 1 What is the average of these weights? mean(x) ## [1] 23.89338 2.0.2 Question 2 After setting the seed at 1, set.seed(1) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(1) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 0.2706222 2.0.3 Question 3 After setting the seed at 5, set.seed(5) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(5) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 1.433378 2.0.4 Question 4 Why are the answers from 2 and 3 different? set.seed(1) # question 2 a &lt;- sample(x,5) a ## Bodyweight60 Bodyweight84 Bodyweight128 Bodyweight202 ## 21.51 28.14 24.04 23.45 ## Bodyweight45 ## 23.68 set.seed(5) # question 3 b &lt;- sample(x,5) b ## Bodyweight46 Bodyweight154 Bodyweight205 Bodyweight64 ## 21.86 20.30 22.95 21.92 ## Bodyweight24 ## 25.27 identical(a,b) # these two samples are not identical ## [1] FALSE Notice that samples a and b differ. Since the seeds were different (1 vs 5), different random numbers were generated. Therefore, the answer is C: Because the average of the samples is a random variable. 2.0.5 Question 5 Set the seed at 1, then using a for-loop take a random sample of 5 mice in 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.498 To make a for loop work in R, an empty vector needs to be created first. This can be achieved with the function vector. In this example, the empty vector is res (short for result). In the for loop, each average (avg_sample) from one repetition gets stored in res. 2.0.6 Question 6 We are now going to increase the number of times we redo the sample from 1,000 to 10,000. Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save these averages. What percent of these 10,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 10000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.4976 2.0.7 Question 7 Note that the answers to 5 and 6 barely changed. This is expected. The way we think about the random value distributions is as the distribution of the list of values obtained if we repeated the experiment an infinite number of times. On a computer, we can’t perform an infinite number of iterations so instead, for our examples, we consider 1,000 to be large enough, thus 10,000 is as well. Now if instead we change the sample size, then we change the random variable and thus its distribution. Set the seed at 1, then using a for-loop take a random sample of 50 mice 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.019 2.0.8 Question 8 Use a histogram to “look” at the distribution of averages we get with a sample size of 5 and sample size of 50. How would you say they differ? # sample size = 5 set.seed(1) n &lt;- 1000 res5 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res5[[i]] &lt;- avg_sample } sd(res5) # standard deviation = spread of the histogram ## [1] 1.52445 # sample size = 50 set.seed(1) n &lt;- 1000 res50 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res50[[i]] &lt;- avg_sample } sd(res50) # standard deviation = spread of the histogram ## [1] 0.4260116 mypar(1,2) # plot histograms hist(res5) hist(res50) mypar is a function from the package rafalib. It helps to align multiple plots in a single plot. mypar(1,1) contains one panel only, mypar(2,1) contains 2 rows of panels and 1 column, mypar(1,2) contains 1 row of panels and 2 columns, etc. Type ?mypar for more information. hist plots a histogram. The answer is B: They both look normal, but with a sample size of 50 the spread is smaller. 2.0.9 Question 9 For the last set of averages, the ones obtained from a sample size of 50, what percent are between 23 and 25? mean((res50 &gt;=23) &amp; (res50 &lt;= 25)) ## [1] 0.976 2.0.10 Question 10 Now ask the same question of a normal distribution with average 23.9 and standard deviation 0.43. pnorm(25,23.9,0.43) - pnorm(23,23.9,0.43) ## [1] 0.9765648 The answers to 9 and 10 were very similar. This is because we can approximate the distribution of the sample average with a normal distribution. We will learn more about the reason for this next. 2.1 Ch 2.9 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) 2.1.1 Question 1 Use dplyr to create a vector x with the body weight of all males on the control (chow) diet. What is this population’s average? x &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mean(x) ## [1] 30.96381 Throughout the book, I will be using %&gt;% for brevity. If you don’t understand it, please check out Chapter 18 of *R for Data Science. 2.1.2 Question 2 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(x) ## [1] 4.420501 2.1.3 Question 3 Set the seed at 1. Take a random sample X of size 25 from x. What is the sample average? set.seed(1) samp_x &lt;- sample(x,25) # sample of x mean(samp_x) ## [1] 32.0956 2.1.4 Question 4 Use dplyr to create a vector y with the body weight of all males on the high fat (hf) diet. What is this population’s average? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mean(y) ## [1] 34.84793 2.1.5 Question 5 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(y) ## [1] 5.574609 2.1.6 Question 6 Set the seed at 1. Take a random sample Y of size 25 from y. What is the sample average? set.seed(1) samp_y &lt;- sample(y,25) mean(samp_y) ## [1] 34.768 2.1.7 Question 7 What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? pop_diff &lt;- mean(y) - mean(x) sample_diff &lt;- mean(samp_y) - mean(samp_x) abs(sample_diff - pop_diff) ## [1] 1.211716 2.1.8 Question 8 Repeat the above for females. Make sure to set the seed to 1 before each sample call. What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? chow_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() # x hf_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() # y set.seed(1) sample_chow_f_pop &lt;- sample(chow_f_pop, 25) # X sample_hf_f_pop &lt;- sample(hf_f_pop,25) # Y pop_diff &lt;- mean(hf_f_pop) - mean(chow_f_pop) # y - x sample_diff &lt;- mean(sample_hf_f_pop) - mean(sample_chow_f_pop) # Y - X abs(sample_diff - pop_diff) ## [1] 0.07888278 2.1.9 Question 9 For the females, our sample estimates were closer to the population difference than with males. What is a possible explanation for this? ans &lt;- c(popsd(hf_f_pop), popsd(chow_f_pop), popsd(y), popsd(x)) names(ans) &lt;- c(&#39;hf female&#39;, &#39;chow female&#39;, &#39;hf male&#39;, &#39;chow male&#39;) ans ## hf female chow female hf male chow male ## 5.069870 3.416438 5.574609 4.420501 The answer is A: The population variance of the females is smaller than that of the males; thus, the sample variable has less variability. 2.2 Ch 2.11 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) 2.2.1 Question 1 If a list of numbers has a distribution that is well approximated by the normal distribution, what proportion of these numbers are within one standard deviation away from the list’s average? pnorm(1) - pnorm(-1) ## [1] 0.6826895 2.2.2 Question 2 What proportion of these numbers are within two standard deviations away from the list’s average? pnorm(2) - pnorm(-2) ## [1] 0.9544997 2.2.3 Question 3 What proportion of these numbers are within three standard deviations away from the list’s average? pnorm(3) - pnorm(-3) ## [1] 0.9973002 2.2.4 Question 4 Define y to be the weights of males on the control diet. What proportion of the mice are within one standard deviation away from the average weight (remember to use popsd for the population sd)? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() z_score &lt;- (y - mean(y))/popsd(y) # get t-statistic (i.e., z score) mean(abs(z_score) &lt;= 1) ## [1] 0.6950673 mean(abs(y - mean(y)) &lt;= popsd(y)) ## [1] 0.6950673 2.2.5 Question 5 What proportion of these numbers are within two standard deviations away from the list’s average? mean(abs(z_score) &lt;= 2) ## [1] 0.9461883 mean(abs(y - mean(y)) &lt;= 2*popsd(y)) ## [1] 0.9461883 2.2.6 Question 6 What proportion of these numbers are within three standard deviations away from the list’s average? mean(abs(z_score) &lt;= 3) ## [1] 0.9910314 mean(abs(y - mean(y)) &lt;= 3*popsd(y)) ## [1] 0.9910314 2.2.7 Question 7 Note that the numbers for the normal distribution and our weights are relatively close. Also, notice that we are indirectly comparing quantiles of the normal distribution to quantiles of the mouse weight distribution. We can actually compare all quantiles using a qq-plot. Which of the following best describes the qq-plot comparing mouse weights to the normal distribution? mypar(1,1) qqnorm(y) qqline(y) The answer is C: The mouse weights are well approximated by the normal distribution, although the larger values (right tail) are larger than predicted by the normal.This is consistent with the differences seen between question 3 and 6. 2.2.8 Question 8 Create the above qq-plot for the four populations: male/females on each of the two diets. What is the most likely explanation for the mouse weights being well approximated? What is the best explanation for all these being well approximated by the normal distribution? mc &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mhf &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() fc &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() fhf &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mypar(2,2) qqnorm(mc, main = &#39;male control pop&#39;) qqline(mc) qqnorm(mhf, main = &#39;male high fat pop&#39;) qqline(mhf) qqnorm(fc, main = &#39;female control pop&#39;) qqline(fc) qqnorm(fhf, main = &#39;female high fat pop&#39;) qqline(fhf) The answer is B: This just happens to be how nature behaves. Perhaps the result of many biological factors averaging out. 2.2.9 Question 9 Here we are going to use the function replicate to learn about the distribution of random variables. All the above exercises relate to the normal distribution as an approximation of the distribution of a fixed list of numbers or a population. We have not yet discussed probability in these exercises. If the distribution of a list of numbers is approximately normal, then if we pick a number at random from this distribution, it will follow a normal distribution. However, it is important to remember that stating that some quantity has a distribution does not necessarily imply this quantity is random. Also, keep in mind that this is not related to the central limit theorem. The central limit applies to averages of random variables. Let’s explore this concept. We will now take a sample of size 25 from the population of males on the chow diet. The average of this sample is our random variable. We will use the replicate to observe 10,000 realizations of this random variable. Set the seed at 1, generate these 10,000 averages. Make a histogram and qq-plot of these 10,000 numbers against the normal distribution. We can see that, as predicted by the CLT, the distribution of the random variable is very well approximated by the normal distribution. y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist avgs &lt;- replicate(10000, mean( sample(y, 25))) mypar(1,2) hist(avgs) qqnorm(avgs) qqline(avgs) What is the average of the distribution of the sample average? m &lt;- 10000 n &lt;- 25 y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist set.seed(1) avg_list &lt;- replicate(m, { mean(sample(y,25)) }) mypar(1,2) hist(avg_list) # distribution qqnorm(avg_list) # qq-plot qqline(avg_list) mean(avg_list) # mean of the sample averages ## [1] 30.95581 2.2.10 Question 10 What is the standard deviation of the distribution of sample averages? popsd(avg_list) ## [1] 0.8368192 2.2.11 Question 11 According to the CLT, the answer to exercise 9 should be the same as mean(y). You should be able to confirm that these two numbers are very close. Which of the following does the CLT tell us should be close to your answer to exercise 10? popsd(y)/sqrt(25) # answer is C ## [1] 0.8841001 2.2.12 Question 12 In practice we do not know \\(\\sigma\\) (popsd(y)) which is why we can’t use the CLT directly. This is because we see a sample and not the entire distribution. We also can’t use popsd(avgs) because to construct averages, we have to take 10,000 samples and this is never practical. We usually just get one sample. Instead we have to estimate popsd(y). As described, what we use is the sample standard deviation. Set the seed at 1, using the replicate function, create 10,000 samples of 25 and now, instead of the sample average, keep the standard deviation. Look at the distribution of the sample standard deviations. It is a random variable. The real population SD is about 4.5. What proportion of the sample SDs are below 3.5? m &lt;- 10000 set.seed(1) sd_list &lt;- replicate(m, { sd(sample(y,25)) }) mypar(1,1) hist(sd_list) mean(sd_list &lt;= 3.5) ## [1] 0.0964 2.2.13 Question 13 What the answer to question 12 reveals is that the denominator of the t-test is a random variable. By decreasing the sample size, you can see how this variability can increase. It therefore adds variability. The smaller the sample size, the more variability is added. The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which of the following is true? x = seq(0.0001, 0.9999, len = 300) df_list &lt;- c(3,10,30,100) mypar(2,2) for (i in seq_along(df_list)) { qqnorm(qt(x,df_list[i]), main = df_list[i]) } The answer is C: The t-distribution has larger tails up until 30 degrees of freedom, at which point it is practically the same as the normal distribution. 2.3 Ch 2.13 Exercises dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleMiceWeights.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) 2.3.1 Question 1 The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let’s try a simple example related to gambling. Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies. We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). n &lt;- 100 B &lt;- 10000 p &lt;- 1/6 set.seed(1) res_list &lt;- replicate(B, { x = sample(1:6,n, replace = T) z &lt;- (mean(x==6)-p) / sqrt(p*(1-p)/n) return(z) }) mean(abs(res_list) &gt; 2) ## [1] 0.0424 2.3.2 Question 2 For the last simulation you can make a qqplot to confirm the normal approximation. Now, the CLT is an asymptotic result, meaning it is closer and closer to being a perfect approximation as the sample size increases. In practice, however, we need to decide if it is appropriate for actual sample sizes. Is 10 enough? 15? 30? In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to “kick in”. Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best? Ps &lt;- c(0.01,0.5) Ns &lt;- c(5,30,100) set.seed(1) question2 &lt;- function(n,p, B = 10000) { res_list &lt;- replicate(B, { sides &lt;- 1/p x = sample(1:sides, n, replace = T) z &lt;- (mean(x==1)-p) / sqrt(p*(1-p)/n) return(z) }) } mypar(2,2) qqnorm(question2(5,0.5), main = &#39;n=5, p=0.5&#39;) qqnorm(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) # the answer is B qqnorm(question2(30,0.01), main = &#39;n=30, p=0.01&#39;) qqnorm(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) mypar(1,2) hist(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) hist(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) 2.3.3 Question 3 As we have already seen, the CLT also applies to averages of quantitative data. A major difference with binary data, for which we know the variance is p(1-p), is that with quantitative data we need to estimate the population standard deviation. In several previous exercises we have illustrated statistical concepts with the unrealistic situation of having access to the entire population. In practice, we do not have access to entire populations. Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations: X &lt;- filter(dat, Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist Y &lt;- filter(dat, Diet==&quot;hf&quot;) %&gt;% select(Bodyweight) %&gt;% unlist We think of X as a random sample from the population of all mice in the control diet and Y as a random sample from the population of all mice in the high fat diet. Define the parameter \\(\\mu_x\\) as the average of the control population. We estimate this parameter with the sample average \\(\\bar{X}\\). What is the sample average? mean(X) ## [1] 23.81333 2.3.4 Question 4 We don’t know \\(\\mu_x\\), but want to use \\(\\bar{X}\\) to understand \\(\\mu_x\\). Which of the following uses CLT to understand how well \\(\\bar{X}\\) approximates \\(\\mu_x\\)? The answer is D: \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_x\\) and standard deviation of \\(\\frac{\\sigma_x}{\\sqrt{12}}\\) where \\(\\sigma_x\\) is the population of standard deviation. 2.3.5 Question 5 The result above tells us the distribution of the following random variable \\(Z = \\sqrt{12}\\frac{\\bar{X}-\\mu_x}{\\sigma_x}\\). What does the CLT tell us is the mean of \\(Z\\) (you don’t need code)? The answer is 0. Z refers to t-statistic (something that the book does not explicitly highlight), which is not raw data. Instead, we use raw data and then compute t-statistic. This is the value that is used to compute p-value based on the normal distribution. In the case of z-score or t-statistic, it is 0 at the mean of the sample due to its mathematical definition. 2.3.6 Question 6 The result of 4 and 5 tell us that we know the distribution of the difference between our estimate and what we want to estimate, but don’t know. However, the equation involves the population standard deviation \\(\\sigma_X\\), which we don’t know. Given what we discussed, what is your estimate of \\(\\sigma_x\\)? sd(X) ## [1] 3.022541 2.3.7 Question 7 Use the CLT to approximate the probability that our estimate \\(\\bar{X}\\) is off by more than 2 grams from \\(\\mu_x\\). z_score &lt;- 2/(sqrt(sd(X)^2/12)) pnorm(-z_score) + 1- pnorm(z_score) ## [1] 0.02189533 2.3.8 Question 8 Now we introduce the concept of a null hypothesis. We don’t know \\(\\mu_x\\) nor \\(mu_y\\). We want to quantify what the data say about the possibility that the diet has no effect: \\(\\mu_x = \\mu_y\\). If we use CLT, then we approximate the distribution of \\(\\bar{X}\\) as normal with mean \\(mu_X\\) and standard deviation of \\(\\frac{\\sigma_X}{\\sqrt{M}}\\) and the distribution of \\(\\bar{Y}\\) and standard deviation of \\(\\frac{\\sigma_y}{\\sqrt{N}}\\), with \\(M\\) and \\(N\\) as the sample sizes for \\(X\\) and \\(Y\\) respectively, in this case 12. This implies that the difference \\(\\bar{Y} - \\bar{X}\\) has mean \\(0\\). We described that the standard deviation of this statistic (the standard error) is \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\) and that we estimate the population standard deviations \\(\\sigma_x\\) and \\(\\sigma_y\\) with the sample estimates. What is the estimate of \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\)? sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 1.469867 2.3.9 Question 9 So now we can compute \\(\\bar{Y}-\\bar{X}\\) as well as an estimate of this standard error and construct a t-statistic. What is this t-statistic? (mean(Y) - mean(X))/ sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 2.055174 2.3.10 Question 10 If we apply the CLT, what is the distribution of this t-statistic? The answer is A: Normal with mean 0 and standard deviation 1. 2.3.11 Question 11 Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed for t-statistic in Question 9, when the null distribution is true? tstat &lt;- (mean(X) - mean(Y))/ sqrt((sd(X)^2 + sd(Y)^2)/12) # from question 9 pnorm(tstat) + 1- pnorm(tstat) ## [1] 1 2.3.12 Question 12 CLT provides an approximation for cases in which the sample size is large. In practice, we can’t check the assumption because we only get to see 1 outcome (which you computed above). As a result, if this approximation is off, so is our p-value. As described earlier, there is another approach that does not require a large sample size, but rather that the distribution of the population is approximately normal. We don’t get to see this distribution so it is again an assumption, although we can look at the distribution of the sample with qqnorm(X) and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows t- distribution. What is the p-value under the t-distribution approximation? Hint: use the t.test function. mypar(1,2) qqnorm(X, main = &#39;Theoretical normal quantiles vs X&#39;) qqline(X) qqnorm(Y, main = &#39;Theoretical normal quantiles vs Y&#39;) qqline(Y) t.test(X,Y) ## ## Welch Two Sample t-test ## ## data: X and Y ## t = -2.0552, df = 20.236, p-value = 0.053 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -6.08463229 0.04296563 ## sample estimates: ## mean of x mean of y ## 23.81333 26.83417 2.3.13 Question 13 With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. They can’t both be right. What best describes the difference? The answer is B: These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation of the standard error and thus, under the null, large values are more probable under the null distribution. 2.4 Ch 2.18 Exercises For these exercises we will load the babies dataset from babies.txt. We will use this data to review the concepts behind the p-values and then test confidence interval concepts. babies &lt;- read.table(&quot;babies.txt&quot;, header=TRUE) This is a large dataset (1,236 cases), and we will pretend that it contains the entire population in which we are interested. We will study the differences in birth weight between babies born to smoking and non-smoking mothers. First, let’s split this into two birth weight datasets: one of birth weights to non-smoking mothers and the other of birth weights to smoking mothers. bwt.nonsmoke &lt;- filter(babies, smoke==0) %&gt;% select(bwt) %&gt;% unlist bwt.smoke &lt;- filter(babies, smoke==1) %&gt;% select(bwt) %&gt;% unlist Now, we can look for the true population difference in means between smoking and non-smoking birth weights. library(rafalib) mean(bwt.nonsmoke)-mean(bwt.smoke) ## [1] 8.937666 popsd(bwt.nonsmoke) ## [1] 17.38696 popsd(bwt.smoke) ## [1] 18.08024 The population difference of mean birth weights is about 8.9 ounces. The standard deviations of the nonsmoking and smoking groups are about 17.4 and 18.1 ounces, respectively. As we did with the mouse weight data, this assessment interactively reviews inference concepts using simulations in R. We will treat the babies dataset as the full population and draw samples from it to simulate individual experiments. We will then ask whether somebody who only received the random samples would be able to draw correct conclusions about the population. We are interested in testing whether the birth weights of babies born to non-smoking mothers are significantly different from the birth weights of babies born to smoking mothers. 2.4.1 Question 1 Set the seed at 1 and obtain two samples, each of size N = 25, from non-smoking mothers (dat.ns) and smoking mothers (dat.s). Compute the t-statistic (call it tval). N &lt;- 25 set.seed(1) dat.ns &lt;- sample(bwt.nonsmoke,N) dat.s &lt;- sample(bwt.smoke,N) tval &lt;- (mean(dat.s) - mean(dat.ns)) / sqrt(var(dat.s)/N + var(dat.ns)/N) t.test(dat.s,dat.ns)$statistic[[1]] ## [1] -2.120904 pt(tval, df = N*2-2)*2 ## [1] 0.03912225 2.4.2 Question 2 Recall that we summarize our data using a t-statistics because we know that in situations where the null hypothesis is true (what we mean when we say “under the null”) and the sample size is relatively large, this t-value will have an approximate standard normal distribution. Because we know the distribution of the t-value under the null, we can quantitatively determine how unusual the observed t-value would be if the null hypothesis were true. The standard procedure is to examine the probability a t-statistic that actually does follow the null hypothesis would have larger absolute value than the absolute value of the t-value we just observed- this is called a two-sided test. We have computed these by taking one minus the area under the standard normal curve between -abs(tval) and abs(tval). In R, we can do this by using the pnorm function, which computes the area under a normal curve from negative infinity up to the value given as its first argument. What is the estimated p-value? (pnorm(tval) + 1 - pnorm(-tval)) ## [1] 0.03392985 2.4.3 Question 3 Because of the symmetry of the standard normal distribution, there is a simpler way to calculate the probability that a t-value under the null could have a larger absolute value than tval. Choose a simplified calculation from the four choices. 1 - 2*pnorm(-abs(tval)) ## [1] 0.9660702 2.4.4 Question 4 By reporting only p-values, many scientific publications provide an incomplete story of their findings. As we have mentioned, with very large sample sizes, scientifically insignificant differences between two groups can lead to small p-values. Confidence intervals are more informative as they include the estimate itself. Our estimate of the difference between babies of smokers and non-smokers: mean(dat.s) - mean( dat.ns). If we use the CLT, what quantity would we add and subtract to this estimate to obtain a 99% confidence interval? mean(dat.s) - mean(dat.ns) ## [1] -9.92 Q &lt;- qnorm(0.5 + 0.99/2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Q*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Q*se) ## [1] -21.967797 2.127797 2.4.5 Question 5 If instead of CLT, we use the t-distribution approximation, what do we add and subtract (use 2*N-2 degrees of freedom)? Qt &lt;- qt(0.5 + 0.99/2, df = N*2-2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Qt*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Qt*se) ## [1] -22.465339 2.625339 t.test(dat.s, dat.ns) ## ## Welch Two Sample t-test ## ## data: dat.s and dat.ns ## t = -2.1209, df = 47.693, p-value = 0.03916 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.3258047 -0.5141953 ## sample estimates: ## mean of x mean of y ## 114.76 124.68 "]
]
