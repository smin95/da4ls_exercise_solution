[
["index.html", "Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Welcome! Acknowledgment Frequently Asked Questions", " Data Analysis for the Life Sciences with R: Exercise Solutions (in progress) Seung Hyun (Sam) Min 2020-12-11 Welcome! This book contains unofficial exercise solutions for the book Data Analysis for the Life Sciences with R by Rafael A. Irizarry and Michael I. Love. The PDF copy of the book is available for free and the physical copy is available in Amazon. Acknowledgment I would like to thank Rafael A. Irizarry and Michael I. Love for writing this wonderful book, and my friends who encouraged me to undertake this project. Frequently Asked Questions You can read the FAQs in the github page. "],
["getting-started.html", "Chapter 1 Getting started", " Chapter 1 Getting started Since this chapter does not deal with statistics, I have decided to skip this chapter altogether. Instead, here are my thoughts on the book and each chapter. Before reading DA4LS with R The book gives a brief introduction to R. But I do not think this is enough. The codes in the later part of the book can get quite complicated (especially from Chapter 6). If you do not know R (or have no coding experience), I suggest you read Chapters 5, 15, 19-21 of R for Data Science by Hadley Wickham and Garrett Grolemund; these chapters discuss data transformation, factors, custom functions, vectors and iterations (ex. for loops, sapply). These concepts are deeply embeded in the codes throughout the book. There is also matrix algebra (ex. singular value decomposition) in this book (from Chapter 4). However, the book reviews matrix algebra briefly. If you have not taken a course in linear algebra, I suggest that you spend some extra time reviewing key concepts such as matrix multiplication, dot product, orthogonal matrix, inverse matrix and square matrix before reading Chapter 4. Here is a link to Khan Academy. When you are going through Chapter 8, you might also have you look up for extra resource on principal component analysis; I recommend this Youtube video by StatQuest. "],
["inference.html", "Chapter 2 Inference 2.7 Exercises 2.9 Exercises 2.11 Exercises 2.13 Exercises 2.18 Exercises 2.21 Exercises 2.23 Exercises 2.25 Exercises", " Chapter 2 Inference Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the functions filter() and %&gt;% library(rafalib) # important for plotting with base R 2.7 Exercises If you have not downloaded the data before, dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleControlsPopulation.csv&quot; url &lt;- paste0(dir, filename) x &lt;- unlist(read.csv(url)) Or if you already have downloaded the data, then just upload it. dat &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) bodyweight &lt;- select(dat, Bodyweight) x &lt;- unlist(bodyweight) # or use pipe %&gt;% x &lt;- read.csv(&#39;femaleControlsPopulation.csv&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() Check out what unlist does by typing ?unlist in the command. The second method is more concise because of the pipe %&gt;%, which allows multiple lines of commands to be in one continuous line. Question 1 What is the average of these weights? mean(x) ## [1] 23.89338 Question 2 After setting the seed at 1, set.seed(1) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(1) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 0.2706222 Question 3 After setting the seed at 5, set.seed(5) take a random sample size 5. What is the absolute value (use abs) of the difference between the average of the sample and the average of all the values? set.seed(5) avg_sample &lt;- mean(sample(x,5)) # average of the sample of 5 avg_pop &lt;- mean(x) # average of all values abs(avg_sample - avg_pop) # absolute difference ## [1] 1.433378 Question 4 Why are the answers from 2 and 3 different? set.seed(1) # question 2 a &lt;- sample(x,5) a ## Bodyweight60 Bodyweight84 Bodyweight128 Bodyweight202 ## 21.51 28.14 24.04 23.45 ## Bodyweight45 ## 23.68 set.seed(5) # question 3 b &lt;- sample(x,5) b ## Bodyweight46 Bodyweight154 Bodyweight205 Bodyweight64 ## 21.86 20.30 22.95 21.92 ## Bodyweight24 ## 25.27 identical(a,b) # these two samples are not identical ## [1] FALSE Notice that samples a and b differ. Since the seeds were different (1 vs 5), different random numbers were generated. Therefore, the answer is C: Because the average of the samples is a random variable. Question 5 Set the seed at 1, then using a for-loop take a random sample of 5 mice in 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.498 To make a for loop work in R, an empty vector needs to be created first. This can be achieved with the function vector. In this example, the empty vector is res (short for result). In the for loop, each average (avg_sample) from one repetition gets stored in res. Question 6 We are now going to increase the number of times we redo the sample from 1,000 to 10,000. Set the seed at 1, then using a for-loop take a random sample of 5 mice 10,000 times. Save these averages. What percent of these 10,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 10000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.4976 Question 7 Note that the answers to 5 and 6 barely changed. This is expected. The way we think about the random value distributions is as the distribution of the list of values obtained if we repeated the experiment an infinite number of times. On a computer, we can’t perform an infinite number of iterations so instead, for our examples, we consider 1,000 to be large enough, thus 10,000 is as well. Now if instead we change the sample size, then we change the random variable and thus its distribution. Set the seed at 1, then using a for-loop take a random sample of 50 mice 1,000 times. Save these averages. What percent of these 1,000 averages are more than 1 gram away from the average of x? set.seed(1) n &lt;- 1000 res &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res[[i]] &lt;- avg_sample } mean(abs(res-mean(x)) &gt; 1) ## [1] 0.019 Question 8 Use a histogram to “look” at the distribution of averages we get with a sample size of 5 and sample size of 50. How would you say they differ? # sample size = 5 set.seed(1) n &lt;- 1000 res5 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,5)) res5[[i]] &lt;- avg_sample } sd(res5) # standard deviation = spread of the histogram ## [1] 1.52445 # sample size = 50 set.seed(1) n &lt;- 1000 res50 &lt;- vector(&#39;double&#39;,n) for (i in seq(n)) { avg_sample &lt;- mean(sample(x,50)) res50[[i]] &lt;- avg_sample } sd(res50) # standard deviation = spread of the histogram ## [1] 0.4260116 The standard deviation of res50 is smaller than that of res5 because of the difference in the sample sizes. A higher standard deviation leads to a wider histogram. See two figures below. mypar(1,2) # plot histograms hist(res5) hist(res50) mypar is a function from the package rafalib. It helps to align multiple plots in a single plot. mypar(1,1) contains one panel only, mypar(2,1) contains 2 rows of panels and 1 column, mypar(1,2) contains 1 row of panels and 2 columns, etc. Type ?mypar for more information. You will be using this function to plot a graph throughout the entire book. hist plots a histogram. The answer is B: They both look normal, but with a sample size of 50 the spread is smaller. Question 9 For the last set of averages, the ones obtained from a sample size of 50, what percent are between 23 and 25? mean((res50 &gt;=23) &amp; (res50 &lt;= 25)) ## [1] 0.976 Question 10 Now ask the same question of a normal distribution with average 23.9 and standard deviation 0.43. pnorm(25,23.9,0.43) - pnorm(23,23.9,0.43) ## [1] 0.9765648 The answers to 9 and 10 were very similar. This is because we can approximate the distribution of the sample average with a normal distribution. We will learn more about the reason for this next. 2.9 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) Question 1 Use dplyr to create a vector x with the body weight of all males on the control (chow) diet. What is this population’s average? x &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mean(x) ## [1] 30.96381 Throughout the book, I will be using %&gt;% for brevity. If you don’t understand it, please check out Chapter 18 of *R for Data Science. Question 2 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(x) ## [1] 4.420501 popsd and sd are slightly different. sd calculates the standard deviation of the sample size, so the denominator that it uses to compute SD is n-1. Function var also uses denominator n-1 to calculate variance. However, popsd (which is from rafalib package) uses denominator n. Question 3 Set the seed at 1. Take a random sample X of size 25 from x. What is the sample average? set.seed(1) samp_x &lt;- sample(x,25) # sample of x mean(samp_x) ## [1] 32.0956 Question 4 Use dplyr to create a vector y with the body weight of all males on the high fat (hf) diet. What is this population’s average? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mean(y) ## [1] 34.84793 Question 5 Now use the rafalib package and use the popsd function to compute the population standard deviation. popsd(y) ## [1] 5.574609 Question 6 Set the seed at 1. Take a random sample Y of size 25 from y. What is the sample average? set.seed(1) samp_y &lt;- sample(y,25) mean(samp_y) ## [1] 34.768 Question 7 What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? pop_diff &lt;- mean(y) - mean(x) sample_diff &lt;- mean(samp_y) - mean(samp_x) abs(sample_diff - pop_diff) ## [1] 1.211716 Question 8 Repeat the above for females. Make sure to set the seed to 1 before each sample call. What is the difference in absolute value between \\(\\bar{y}-\\bar{x}\\) and \\(\\bar{Y}-\\bar{X}\\)? chow_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() # x hf_f_pop &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() # y set.seed(1) sample_chow_f_pop &lt;- sample(chow_f_pop, 25) # X set.seed(1) sample_hf_f_pop &lt;- sample(hf_f_pop,25) # Y pop_diff &lt;- mean(hf_f_pop) - mean(chow_f_pop) # y - x sample_diff &lt;- mean(sample_hf_f_pop) - mean(sample_chow_f_pop) # Y - X abs(sample_diff - pop_diff) ## [1] 0.7364828 Question 9 For the females, our sample estimates were closer to the population difference than with males. What is a possible explanation for this? ans &lt;- c(popsd(hf_f_pop), popsd(chow_f_pop), popsd(y), popsd(x)) names(ans) &lt;- c(&#39;hf female&#39;, &#39;chow female&#39;, &#39;hf male&#39;, &#39;chow male&#39;) ans ## hf female chow female hf male chow male ## 5.069870 3.416438 5.574609 4.420501 The answer is A: The population variance of the females is smaller than that of the males; thus, the sample variable has less variability. 2.11 Exercises If you have not downloaded the data before: dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;mice_pheno.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) dat &lt;- na.omit(dat) If you have the data already in your directory: raw_data &lt;- read.csv(&#39;mice_pheno.csv&#39;) dat &lt;- na.omit(raw_data) Question 1 If a list of numbers has a distribution that is well approximated by the normal distribution, what proportion of these numbers are within one standard deviation away from the list’s average? pnorm(1) - pnorm(-1) ## [1] 0.6826895 Question 2 What proportion of these numbers are within two standard deviations away from the list’s average? pnorm(2) - pnorm(-2) ## [1] 0.9544997 Question 3 What proportion of these numbers are within three standard deviations away from the list’s average? pnorm(3) - pnorm(-3) ## [1] 0.9973002 Question 4 Define y to be the weights of males on the control diet. What proportion of the mice are within one standard deviation away from the average weight (remember to use popsd for the population sd)? y &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() z_score &lt;- (y - mean(y))/popsd(y) # get t-statistic (i.e., z score) mean(abs(z_score) &lt;= 1) ## [1] 0.6950673 mean(abs(y - mean(y)) &lt;= popsd(y)) ## [1] 0.6950673 It doesn’t matter which solution you use as long as you have the same answer. Question 5 What proportion of these numbers are within two standard deviations away from the list’s average? mean(abs(z_score) &lt;= 2) ## [1] 0.9461883 mean(abs(y - mean(y)) &lt;= 2*popsd(y)) ## [1] 0.9461883 It doesn’t matter which solution you use as long as you have the same answer. Question 6 What proportion of these numbers are within three standard deviations away from the list’s average? mean(abs(z_score) &lt;= 3) ## [1] 0.9910314 mean(abs(y - mean(y)) &lt;= 3*popsd(y)) ## [1] 0.9910314 It doesn’t matter which solution you use as long as you have the same answer. Question 7 Note that the numbers for the normal distribution and our weights are relatively close. Also, notice that we are indirectly comparing quantiles of the normal distribution to quantiles of the mouse weight distribution. We can actually compare all quantiles using a qq-plot. Which of the following best describes the qq-plot comparing mouse weights to the normal distribution? mypar(1,1) qqnorm(y) qqline(y) The answer is C: The mouse weights are well approximated by the normal distribution, although the larger values (right tail) are larger than predicted by the normal.This is consistent with the differences seen between question 3 and 6. Question 8 Create the above qq-plot for the four populations: male/females on each of the two diets. What is the most likely explanation for the mouse weights being well approximated? What is the best explanation for all these being well approximated by the normal distribution? mc &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mhf &lt;- dat %&gt;% filter(Sex == &#39;M&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() fc &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;chow&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() fhf &lt;- y &lt;- dat %&gt;% filter(Sex == &#39;F&#39; &amp; Diet == &#39;hf&#39;) %&gt;% select(Bodyweight) %&gt;% unlist() mypar(2,2) qqnorm(mc, main = &#39;male control pop&#39;) qqline(mc) qqnorm(mhf, main = &#39;male high fat pop&#39;) qqline(mhf) qqnorm(fc, main = &#39;female control pop&#39;) qqline(fc) qqnorm(fhf, main = &#39;female high fat pop&#39;) qqline(fhf) The answer is B: This just happens to be how nature behaves. Perhaps the result of many biological factors averaging out. Question 9 Here we are going to use the function replicate to learn about the distribution of random variables. All the above exercises relate to the normal distribution as an approximation of the distribution of a fixed list of numbers or a population. We have not yet discussed probability in these exercises. If the distribution of a list of numbers is approximately normal, then if we pick a number at random from this distribution, it will follow a normal distribution. However, it is important to remember that stating that some quantity has a distribution does not necessarily imply this quantity is random. Also, keep in mind that this is not related to the central limit theorem. The central limit applies to averages of random variables. Let’s explore this concept. We will now take a sample of size 25 from the population of males on the chow diet. The average of this sample is our random variable. We will use the replicate to observe 10,000 realizations of this random variable. Set the seed at 1, generate these 10,000 averages. Make a histogram and qq-plot of these 10,000 numbers against the normal distribution. We can see that, as predicted by the CLT, the distribution of the random variable is very well approximated by the normal distribution. y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist avgs &lt;- replicate(10000, mean( sample(y, 25))) mypar(1,2) hist(avgs) qqnorm(avgs) qqline(avgs) What is the average of the distribution of the sample average? m &lt;- 10000 n &lt;- 25 y &lt;- filter(dat, Sex==&quot;M&quot; &amp; Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist set.seed(1) avg_list &lt;- replicate(m, { mean(sample(y,25)) }) mypar(1,2) hist(avg_list) # distribution qqnorm(avg_list) # qq-plot qqline(avg_list) mean(avg_list) # mean of the sample averages ## [1] 30.95581 Question 10 What is the standard deviation of the distribution of sample averages? popsd(avg_list) ## [1] 0.8368192 Question 11 According to the CLT, the answer to exercise 9 should be the same as mean(y). You should be able to confirm that these two numbers are very close. Which of the following does the CLT tell us should be close to your answer to exercise 10? popsd(y)/sqrt(25) # answer is C ## [1] 0.8841001 Question 12 In practice we do not know \\(\\sigma\\) (popsd(y)) which is why we can’t use the CLT directly. This is because we see a sample and not the entire distribution. We also can’t use popsd(avgs) because to construct averages, we have to take 10,000 samples and this is never practical. We usually just get one sample. Instead we have to estimate popsd(y). As described, what we use is the sample standard deviation. Set the seed at 1, using the replicate function, create 10,000 samples of 25 and now, instead of the sample average, keep the standard deviation. Look at the distribution of the sample standard deviations. It is a random variable. The real population SD is about 4.5. What proportion of the sample SDs are below 3.5? m &lt;- 10000 set.seed(1) sd_list &lt;- replicate(m, { sd(sample(y,25)) }) mypar(1,1) hist(sd_list) mean(sd_list &lt;= 3.5) ## [1] 0.0964 Question 13 What the answer to question 12 reveals is that the denominator of the t-test is a random variable. By decreasing the sample size, you can see how this variability can increase. It therefore adds variability. The smaller the sample size, the more variability is added. The normal distribution stops providing a useful approximation. When the distribution of the population values is approximately normal, as it is for the weights, the t-distribution provides a better approximation. We will see this later on. Here we will look at the difference between the t-distribution and normal. Use the function qt and qnorm to get the quantiles of x=seq(0.0001,0.9999,len=300). Do this for degrees of freedom 3, 10, 30, and 100. Which of the following is true? x = seq(0.0001, 0.9999, len = 300) df_list &lt;- c(3,10,30,100) mypar(2,2) for (i in seq_along(df_list)) { qqnorm(qt(x,df_list[i]), main = df_list[i]) } The answer is C: The t-distribution has larger tails up until 30 degrees of freedom, at which point it is practically the same as the normal distribution. 2.13 Exercises dir &lt;- &quot;https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/&quot; filename &lt;- &quot;femaleMiceWeights.csv&quot; url &lt;- paste0(dir, filename) dat &lt;- read.csv(url) Question 1 The CLT is a result from probability theory. Much of probability theory was originally inspired by gambling. This theory is still used in practice by casinos. For example, they can estimate how many people need to play slots for there to be a 99.9999% probability of earning enough money to cover expenses. Let’s try a simple example related to gambling. Suppose we are interested in the proportion of times we see a 6 when rolling n=100 die. This is a random variable which we can simulate with x=sample(1:6, n, replace=TRUE) and the proportion we are interested in can be expressed as an average: mean(x==6). Because the die rolls are independent, the CLT applies. We want to roll n dice 10,000 times and keep these proportions. This random variable (proportion of 6s) has mean p=1/6 and variance p*(1-p)/n. So according to CLT z = (mean(x==6) - p) / sqrt(p*(1-p)/n) should be normal with mean 0 and SD 1. Set the seed to 1, then use replicate to perform the simulation, and report what proportion of times z was larger than 2 in absolute value (CLT says it should be about 0.05). n &lt;- 100 B &lt;- 10000 p &lt;- 1/6 set.seed(1) res_list &lt;- replicate(B, { x = sample(1:6,n, replace = T) z &lt;- (mean(x==6)-p) / sqrt(p*(1-p)/n) return(z) }) mean(abs(res_list) &gt; 2) ## [1] 0.0424 Question 2 For the last simulation you can make a qqplot to confirm the normal approximation. Now, the CLT is an asymptotic result, meaning it is closer and closer to being a perfect approximation as the sample size increases. In practice, however, we need to decide if it is appropriate for actual sample sizes. Is 10 enough? 15? 30? In the example used in exercise 1, the original data is binary (either 6 or not). In this case, the success probability also affects the appropriateness of the CLT. With very low probabilities, we need larger sample sizes for the CLT to “kick in”. Run the simulation from exercise 1, but for different values of p and n. For which of the following is the normal approximation best? Ps &lt;- c(0.01,0.5) Ns &lt;- c(5,30,100) set.seed(1) question2 &lt;- function(n,p, B = 10000) { res_list &lt;- replicate(B, { sides &lt;- 1/p x = sample(1:sides, n, replace = T) z &lt;- (mean(x==1)-p) / sqrt(p*(1-p)/n) return(z) }) } mypar(2,2) qqnorm(question2(5,0.5), main = &#39;n=5, p=0.5&#39;) qqnorm(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) # the answer is B qqnorm(question2(30,0.01), main = &#39;n=30, p=0.01&#39;) qqnorm(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) mypar(1,2) hist(question2(30,0.5), main = &#39;n=30, p=0.5&#39;) hist(question2(100,0.01), main = &#39;n=100, p=0.01&#39;) The answer is B, n = 30, p = 0.5. I created a custom function for this question. However, another approach (and maybe more concise) is to use a for loop. This is tricky question, primarily because not all dice has number 6 (as is the case in Question 1, mean(x==6)). So you had to calculate the probability where the dice ends up in another side by modifying the code; since all dice has sdie 1, mean(x==1) seems like a safe choice. Also, the number of the sides for each die changes since it is defined by the p value. For instance, if p equals 0.01 for side 1, then there are 100 sides total (1/0.01 = 100). Question 3 As we have already seen, the CLT also applies to averages of quantitative data. A major difference with binary data, for which we know the variance is p(1-p), is that with quantitative data we need to estimate the population standard deviation. In several previous exercises we have illustrated statistical concepts with the unrealistic situation of having access to the entire population. In practice, we do not have access to entire populations. Instead, we obtain one random sample and need to reach conclusions analyzing that data. dat is an example of a typical simple dataset representing just one sample. We have 12 measurements for each of two populations: X &lt;- filter(dat, Diet==&quot;chow&quot;) %&gt;% select(Bodyweight) %&gt;% unlist Y &lt;- filter(dat, Diet==&quot;hf&quot;) %&gt;% select(Bodyweight) %&gt;% unlist We think of X as a random sample from the population of all mice in the control diet and Y as a random sample from the population of all mice in the high fat diet. Define the parameter \\(\\mu_x\\) as the average of the control population. We estimate this parameter with the sample average \\(\\bar{X}\\). What is the sample average? mean(X) ## [1] 23.81333 Question 4 We don’t know \\(\\mu_x\\), but want to use \\(\\bar{X}\\) to understand \\(\\mu_x\\). Which of the following uses CLT to understand how well \\(\\bar{X}\\) approximates \\(\\mu_x\\)? The answer is D: \\(\\bar{X}\\) follows a normal distribution with mean \\(\\mu_x\\) and standard deviation of \\(\\frac{\\sigma_x}{\\sqrt{12}}\\) where \\(\\sigma_x\\) is the population of standard deviation. It is important to be aware of the fact that \\(\\bar{X}\\) is based on raw data. So the mean of the distribution can be approximated by the population mean \\(\\mu_x\\). However, if the question asked about z-score, then it would be at 0, because z-score (i.e., t-statistic) is standardized. Question 5 The result above tells us the distribution of the following random variable \\(Z = \\sqrt{12}\\frac{\\bar{X}-\\mu_x}{\\sigma_x}\\). What does the CLT tell us is the mean of \\(Z\\) (you don’t need code)? The answer is 0. Z refers to t-statistic (something that the book does not explicitly highlight), which is not raw data. Instead, we use raw data and then compute t-statistic. This is the value that is used to compute p-value based on the normal distribution. In the case of z-score or t-statistic, it is 0 at the mean of the sample due to its mathematical definition. Question 6 The result of 4 and 5 tell us that we know the distribution of the difference between our estimate and what we want to estimate, but don’t know. However, the equation involves the population standard deviation \\(\\sigma_X\\), which we don’t know. Given what we discussed, what is your estimate of \\(\\sigma_x\\)? sd(X) ## [1] 3.022541 Question 7 Use the CLT to approximate the probability that our estimate \\(\\bar{X}\\) is off by more than 2 grams from \\(\\mu_x\\). z_score &lt;- 2/(sqrt(sd(X)^2/12)) pnorm(-z_score) + 1- pnorm(z_score) ## [1] 0.02189533 Question 8 Now we introduce the concept of a null hypothesis. We don’t know \\(\\mu_x\\) nor \\(mu_y\\). We want to quantify what the data say about the possibility that the diet has no effect: \\(\\mu_x = \\mu_y\\). If we use CLT, then we approximate the distribution of \\(\\bar{X}\\) as normal with mean \\(mu_X\\) and standard deviation of \\(\\frac{\\sigma_X}{\\sqrt{M}}\\) and the distribution of \\(\\bar{Y}\\) and standard deviation of \\(\\frac{\\sigma_y}{\\sqrt{N}}\\), with \\(M\\) and \\(N\\) as the sample sizes for \\(X\\) and \\(Y\\) respectively, in this case 12. This implies that the difference \\(\\bar{Y} - \\bar{X}\\) has mean \\(0\\). We described that the standard deviation of this statistic (the standard error) is \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\) and that we estimate the population standard deviations \\(\\sigma_x\\) and \\(\\sigma_y\\) with the sample estimates. What is the estimate of \\(SE(\\bar{Y} - \\bar{X}) = \\sqrt{\\sigma_y^2/12 + \\sigma_x^2/12}\\)? sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 1.469867 Question 9 So now we can compute \\(\\bar{Y}-\\bar{X}\\) as well as an estimate of this standard error and construct a t-statistic. What is this t-statistic? (mean(Y) - mean(X))/ sqrt((sd(X)^2 + sd(Y)^2)/12) ## [1] 2.055174 This is a good formula to memorize because it will return in later chapters. Knowing this formula can enable us to compute p-values from scratch. Question 10 If we apply the CLT, what is the distribution of this t-statistic? The answer is A: Normal with mean 0 and standard deviation 1. T-statistic is z-score, so the values are standardized to the sample mean. Therefore, the mean is 0. Question 11 Now we are ready to compute a p-value using the CLT. What is the probability of observing a quantity as large as what we computed for t-statistic in Question 9, when the null distribution is true? tstat &lt;- (mean(Y) - mean(X))/ sqrt((sd(X)^2 + sd(Y)^2)/12) # from question 9 2*(1-pnorm(tstat)) ## [1] 0.0398622 Question 12 CLT provides an approximation for cases in which the sample size is large. In practice, we can’t check the assumption because we only get to see 1 outcome (which you computed above). As a result, if this approximation is off, so is our p-value. As described earlier, there is another approach that does not require a large sample size, but rather that the distribution of the population is approximately normal. We don’t get to see this distribution so it is again an assumption, although we can look at the distribution of the sample with qqnorm(X) and qqnorm(Y). If we are willing to assume this, then it follows that the t-statistic follows t- distribution. What is the p-value under the t-distribution approximation? Hint: use the t.test function. mypar(1,2) qqnorm(X, main = &#39;Theoretical normal quantiles vs X&#39;) qqline(X) qqnorm(Y, main = &#39;Theoretical normal quantiles vs Y&#39;) qqline(Y) t.test(X,Y)$p.value ## [1] 0.05299888 Question 13 With the CLT distribution, we obtained a p-value smaller than 0.05 and with the t-distribution, one that is larger. They can’t both be right. What best describes the difference? The answer is B: These are two different assumptions. The t-distribution accounts for the variability introduced by the estimation of the standard error and thus, under the null, large values are more probable under the null distribution. 2.18 Exercises For these exercises we will load the babies dataset from babies.txt. We will use this data to review the concepts behind the p-values and then test confidence interval concepts. babies &lt;- read.table(&quot;babies.txt&quot;, header=TRUE) This is a large dataset (1,236 cases), and we will pretend that it contains the entire population in which we are interested. We will study the differences in birth weight between babies born to smoking and non-smoking mothers. First, let’s split this into two birth weight datasets: one of birth weights to non-smoking mothers and the other of birth weights to smoking mothers. bwt.nonsmoke &lt;- filter(babies, smoke==0) %&gt;% select(bwt) %&gt;% unlist bwt.smoke &lt;- filter(babies, smoke==1) %&gt;% select(bwt) %&gt;% unlist Now, we can look for the true population difference in means between smoking and non-smoking birth weights. library(rafalib) mean(bwt.nonsmoke)-mean(bwt.smoke) ## [1] 8.937666 popsd(bwt.nonsmoke) ## [1] 17.38696 popsd(bwt.smoke) ## [1] 18.08024 The population difference of mean birth weights is about 8.9 ounces. The standard deviations of the nonsmoking and smoking groups are about 17.4 and 18.1 ounces, respectively. As we did with the mouse weight data, this assessment interactively reviews inference concepts using simulations in R. We will treat the babies dataset as the full population and draw samples from it to simulate individual experiments. We will then ask whether somebody who only received the random samples would be able to draw correct conclusions about the population. We are interested in testing whether the birth weights of babies born to non-smoking mothers are significantly different from the birth weights of babies born to smoking mothers. Question 1 Set the seed at 1 and obtain two samples, each of size N = 25, from non-smoking mothers (dat.ns) and smoking mothers (dat.s). Compute the t-statistic (call it tval). N &lt;- 25 set.seed(1) dat.ns &lt;- sample(bwt.nonsmoke,N) dat.s &lt;- sample(bwt.smoke,N) tval &lt;- (mean(dat.s) - mean(dat.ns)) / sqrt(var(dat.s)/N + var(dat.ns)/N) t.test(dat.s,dat.ns)$statistic[[1]] ## [1] -2.120904 pt(tval, df = N*2-2)*2 ## [1] 0.03912225 Question 2 Recall that we summarize our data using a t-statistics because we know that in situations where the null hypothesis is true (what we mean when we say “under the null”) and the sample size is relatively large, this t-value will have an approximate standard normal distribution. Because we know the distribution of the t-value under the null, we can quantitatively determine how unusual the observed t-value would be if the null hypothesis were true. The standard procedure is to examine the probability a t-statistic that actually does follow the null hypothesis would have larger absolute value than the absolute value of the t-value we just observed- this is called a two-sided test. We have computed these by taking one minus the area under the standard normal curve between -abs(tval) and abs(tval). In R, we can do this by using the pnorm function, which computes the area under a normal curve from negative infinity up to the value given as its first argument. What is the estimated p-value? (pnorm(tval) + 1 - pnorm(-tval)) ## [1] 0.03392985 Question 3 Because of the symmetry of the standard normal distribution, there is a simpler way to calculate the probability that a t-value under the null could have a larger absolute value than tval. Choose a simplified calculation from the four choices. 2*pnorm(-abs(tval)) ## [1] 0.03392985 Question 4 By reporting only p-values, many scientific publications provide an incomplete story of their findings. As we have mentioned, with very large sample sizes, scientifically insignificant differences between two groups can lead to small p-values. Confidence intervals are more informative as they include the estimate itself. Our estimate of the difference between babies of smokers and non-smokers: mean(dat.s) - mean( dat.ns). If we use the CLT, what quantity would we add and subtract to this estimate to obtain a 99% confidence interval? mean(dat.s) - mean(dat.ns) ## [1] -9.92 Q &lt;- qnorm(0.5 + 0.99/2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Q*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Q*se) ## [1] -21.967797 2.127797 print(Q*se) ## [1] 12.0478 Question 5 If instead of CLT, we use the t-distribution approximation, what do we add and subtract (use 2*N-2 degrees of freedom)? Qt &lt;- qt(0.5 + 0.99/2, df = N*2-2) se &lt;- sqrt(var(dat.ns)/N + var(dat.s)/N) c(-Qt*se + mean(dat.s) - mean(dat.ns), mean(dat.s) - mean(dat.ns) + Qt*se) ## [1] -22.465339 2.625339 t.test(dat.s, dat.ns) ## ## Welch Two Sample t-test ## ## data: dat.s and dat.ns ## t = -2.1209, df = 47.693, p-value = 0.03916 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -19.3258047 -0.5141953 ## sample estimates: ## mean of x mean of y ## 114.76 124.68 print(Qt*se) ## [1] 12.54534 Question 6 Why are the values from 4 and 5 so similar? The answer is C: N and thus the degrees of freedom is large enough to make the normal and t-distributions very similar. Question 7 Which of the following sentences about a Type I error is false? The answer is C: From the original data alone, you can tell whether you have made a Type I error. Question 8 Set the seed at 1 and take a random sample of \\(N = 5\\) measurements from each of the smoking and nonsmoking datasets. What is the p-value (use the t-test function)? N &lt;- 5 set.seed(1) ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) t.test(ns_sample, s_sample)$p.value ## [1] 0.1366428 Question 9 The p-value is larger than 0.05 so using the typical cut-off, we would not reject. This is a type II error (false negative). Which of the following is not a way to decrease this type of error? The answer is C: Find a population for which the null is not true. Question 10 Set the seed at 1, then use the replicate function to repeat the code in Exercise 9 10,000 times. What proportion of the time do we reject at the 0.05 level? B &lt;- 10000 alpha &lt;- 0.05 set.seed(1) N &lt;- 5 res_list&lt;- replicate(B, { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(alpha &gt; pval) # this returns logical (i.e., TRUE if pval is smaller than 0.05 = alpha), and stores in the vector res_list. }) mean(res_list) ## [1] 0.0984 Question 11 Note that, not surprisingly, the power is lower than 10%. Repeat the exercise above for sample sizes of 30, 60, 90 and 120. Which of these four gives you power of about 80%? pval_calc &lt;- function(N) { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(pval) } Ns &lt;- c(30,60,90,120) B &lt;- 10000 alpha &lt;- 0.05 res_list &lt;- vector(&#39;double&#39;,length(Ns)) for (i in seq_along(Ns)) { res_list[[i]] &lt;- mean(replicate(B, pval_calc(Ns[[i]])) &lt; alpha) } names(res_list) &lt;- Ns print(res_list) # sample size of 60 gives power of 80% ## 30 60 90 120 ## 0.4933 0.7868 0.9344 0.9835 Sample size of 60 gives power of 80%. Question 12 Repeat Question 11, but now require an \\(\\alpha\\) level of 0.01. Which of these four gives you power of 80%? pval_calc &lt;- function(N) { ns_sample &lt;- sample(bwt.nonsmoke, N) s_sample &lt;- sample(bwt.smoke, N) pval &lt;- t.test(ns_sample, s_sample)$p.value return(pval) } Ns &lt;- c(30,60,90,120) B &lt;- 10000 alpha &lt;- 0.01 res_list &lt;- vector(&#39;double&#39;,length(Ns)) for (i in seq_along(Ns)) { res_list[[i]] &lt;- mean(replicate(B, pval_calc(Ns[[i]])) &lt; alpha) } names(res_list) &lt;- Ns print(res_list) # sample size of 90 gives power of 80% ## 30 60 90 120 ## 0.2461 0.5607 0.7932 0.9241 Sample size of 90 gives power of 80%. 2.21 Exercises We have used Monte Carlo simulation throughout this chapter to demonstrate statistical concepts; namely, sampling from the population. We mostly applied this to demonstrate the statistical properties related to inference on differences in averages. Here, we will consider examples of how Monte Carlo simulations are used in practice. Question 1 Imagine you are William Sealy Gosset and have just mathematically derived the distribution of the t-statistic when the sample comes from a normal distribution. Unlike Gosset you have access to computers and can use them to check the results. Let’s start by creating an outcome. Set the seed at 1, use rnorm to generate a random sample of size 5, \\(X_1,...,.X_5\\) from a standard normal distribution, then compute the t-statistic \\(t = \\sqrt{5}\\bar{X}/s\\) with \\(s\\) the sample standard deviation. What value do you observe? set.seed(1) n &lt;- 5 sample1 &lt;- rnorm(n) tstat &lt;- sqrt(5)*mean(sample1)/sd(sample1) tstat ## [1] 0.3007746 Question 2 You have just performed a Monte Carlo simulation using rnorm, a random number generator for normally distributed data. Gosset’s mathematical calculation tells us that the t-statistic defined in the previous exercise, a random variable, follows a t-distribution with \\(N-1\\) degrees of freedom. Monte Carlo simulations can be used to check the theory: we generate many outcomes and compare them to the theoretical result. Set the seed to 1, generate B = 1000 t-statistics as done in Question 1. What proportion is larger than 2? set.seed(1) samp &lt;- rnorm(5) tstat &lt;- sqrt(5)*samp/sd(samp) get_t &lt;- function(n) { samp &lt;- rnorm(n) tstat &lt;- sqrt(n)*mean(samp)/sd(samp) return(tstat) } set.seed(1) res_list &lt;- replicate(1000, get_t(5)) mean(res_list &gt; 2) ## [1] 0.068 Question 3 The answer to Question 2 is very similar to the theoretical prediction: 1-pt(2,df=4). We can check several such quantiles using qqplot function. To obtain quantiles for the t-distribution we can generate percentiles from just above 0 to just below 1: B=100; ps = seq(1/(B+1), 1-1/(B+1), len = B) and compute the quantiles with qt(ps, df=4). Now we can use qqplot to compare these theoretical quantiles to those obtained in Monte Carlo simulation. Use Monte Carlo simulation developed for Question 2 to corroborate that the t-statistic \\(t = \\sqrt{N}\\bar{X}/s\\) follows a t-distribution for several values of N. For which sample sizes does the approximation best work? B &lt;- 1000 ps = seq(1/(B+1), 1-1/(B+1),len=B) get_t &lt;- function(n) { samp &lt;- rnorm(n) tstat &lt;- sqrt(n)*mean(samp)/sd(samp) return(tstat) } Ns &lt;- c(5,10,50,100) mypar(2,2) set.seed(1) for (i in seq_along(Ns)) { res_list &lt;- replicate(1000, get_t(Ns[i])) theory_t &lt;- qt(ps,df=Ns[i]-1) qqplot(theory_t, res_list, main = paste0(&#39;sample size= &#39;,Ns[i]), xlab = &#39;theory&#39;, ylab = &#39;sim&#39;) abline(0,1) } The approximations are spot on for all sample sizes (answer choice C). Question 4 Use Monte Carlo simulation to corroborate that the t-statistic comparing two means and obtained with normally distributed (mean 0 and sd) data follows a t-distribution. In this case we will use the t.test function with var.equal=TRUE. With this argument the degrees of freedom will be df=2*N-2 with N the sample size. For which sample sizes does the approximation best work? ttestgenerator &lt;- function(n) { cases &lt;- rnorm(n) controls &lt;- rnorm(n) tstat &lt;- t.test(cases,controls)$statistic[[1]] return(tstat) } Ns &lt;- c(5,10,50,100) mypar(2,2) set.seed(1) for (i in seq_along(Ns)) { res_list &lt;- replicate(1000, ttestgenerator(Ns[i])) theory_t &lt;- qt(ps,df=2*Ns[i]-2) qqplot(theory_t, res_list, main = paste0(&#39;sample size= &#39;,Ns[i]), xlab = &#39;theory&#39;, ylab = &#39;sim&#39;) abline(0,1) } The approximations are spot on for all sample sizes (answer choice C). Question 5 Is the following statement true or false? If instead of generating the sample with X = rnorm(15), we generate it with binary data (either positive or negative 1 with probability 0.5) X = sample(c(-1,1),15,replace = TRUE) then the t-statistic tstat &lt;- sqrt(15) * mean(X) / sd(X) is approximated by a t-distribution with 14 degrees of freedom. set.seed(1) res_list &lt;- replicate(1000, { X &lt;- sample(c(-1,1),15,replace = T) tstat &lt;- sqrt(15) * mean(X)/sd(X) return(tstat) }) ps &lt;- seq(1/(B+1), 1-1/(B+1),len=B) theory_t &lt;- qt(ps,df=14) qqplot(res_list, theory_t) False. Instead, it is approximated by binomial distribution because the data are binary (only two values). Question 6 Is the following statement true or false? If instead of generating the sample with X = rnorm(N) with N = 1000, we generate it with binary data X = sample(c(-1,1),15,replace = TRUE) then the t-statistic tstat &lt;- sqrt(15) * mean(X) / sd(X) is approximated by a t-distribution with 999 degrees of freedom. set.seed(1) res_list &lt;- replicate(1000, { X &lt;- sample(c(-1,1),1000,replace = T) tstat &lt;- sqrt(1000) * mean(X)/sd(X) return(tstat) }) B&lt;-1000 ps &lt;- seq(1/(B+1), 1-1/(B+1),len=B) theory_t &lt;- qt(ps,df=999) qqplot(res_list, theory_t) abline(0,1) True. Question 7 We can derive approximation of the distribution of the sample average or the t-statistic theoretically. However, suppose we are interested in the distribution of a statistic for which a theoretical approximation is not immediately obvious. Consider the sample median as an example. Use a Monte Carlo to determine which of the following best approximates the median of a sample taken from normally distributed population with mean 0 and standard deviation 1. The answer is A: Just like for the average, the sample median is approximately normal with mean 0 and SD \\(1/\\sqrt{N}\\). 2.23 Exercises babies &lt;- read.table(&quot;babies.txt&quot;, header=TRUE) bwt.nonsmoke &lt;- filter(babies, smoke==0) %&gt;% select(bwt) %&gt;% unlist bwt.smoke &lt;- filter(babies, smoke==1) %&gt;% select(bwt) %&gt;% unlist Question 1 We will generate the following random variable based on a sample size of 10 and observe the following difference: N=10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke , N) smokers &lt;- sample(bwt.smoke , N) obs &lt;- mean(smokers) - mean(nonsmokers) The question is whether this observed difference is statistically significant. We do not want to rely on the assumptions needed for the normal or t-distribution approximations to hold, so instead we will use permutations. We will reshuffle the data and recompute the mean. We can create one permuted sample with the following code: dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample( dat ) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] mean(smokersstar)-mean(nonsmokersstar) ## [1] -8.5 The last value is one observation from the null distribution we will construct. Set the seed at 1, and then repeat the permutation 1,000 times to create a null distribution. What is the permutation derived p-value for our observation? N &lt;- 10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke,N) smokers &lt;- sample(bwt.smoke,N) obs &lt;- mean(smokers) - mean(nonsmokers) set.seed(1) res_list &lt;- replicate(1000, { dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample(dat) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] avgdiff &lt;- mean(smokersstar) - mean(nonsmokersstar) return(avgdiff) }) (sum(abs(res_list) &gt; abs(obs)) +1)/(length(res_list)+1) ## [1] 0.05294705 Due to the random numbers, the actual answer might differ. Question 2 Repeat the above exercise, but instead of the differences in mean, consider the differences in median obs &lt;- median(smokers) - median(nonsmokers). What is the permutation based p-value? N &lt;- 10 set.seed(1) nonsmokers &lt;- sample(bwt.nonsmoke,N) smokers &lt;- sample(bwt.smoke,N) obs &lt;- median(smokers) - median(nonsmokers) set.seed(1) res_list &lt;- replicate(1000, { dat &lt;- c(smokers,nonsmokers) shuffle &lt;- sample(dat) smokersstar &lt;- shuffle[1:N] nonsmokersstar &lt;- shuffle[(N+1):(2*N)] avgdiff &lt;- median(smokersstar) - median(nonsmokersstar) return(avgdiff) }) (sum(abs(res_list) &gt; abs(obs)) +1)/(length(res_list)+1) ## [1] 0.01798202 Due to the random numbers, the actual answer might differ. 2.25 Exercises d = read.csv(&#39;assoctest.csv&#39;) Question 1 This dataframe refects the allele status (either AA/Aa or aa) and the case/control status for 72 individuals. Compute the Chi-square test for the association of genotype with case/control status (using the table function and the chisq.test function). Examine the table to see if it looks enriched for association by eye. What is the X-squared statistic? tab &lt;- table(d) chisq.test(tab)$statistic ## X-squared ## 3.343653 Question 2 Compute Fisher’s exact test fisher.test for the same table. What is the p-value (two-tailed)? fisher.test(tab)$p.value ## [1] 0.05193834 "],
["exploratory-data-analysis.html", "Chapter 3 Exploratory Data Analysis 3.8 Exercises 3.11 Exercises", " Chapter 3 Exploratory Data Analysis Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the function filter() and %&gt;% library(rafalib) # important for plotting with base R 3.8 Exercises Question 1 Given the above histogram, how many people are between the ages of 35 and 45? 6 people Question 2 The InsectSprays dataset is included in R. The dataset reports the counts of insects in agricultural experimental units treated with different insecticides. Make a boxplot and determine which insecticide appears to be most effective (has the lowest median). dat &lt;- split(InsectSprays$count, InsectSprays$spray) boxplot(dat) C has the lowest median and is, therefore, most effective. Question 3 Download the data and load them by typing load(skew.RData) into R. Use exploratory data analysis tools to determine which two columns are different from the rest. Which column has positive skew (a long tail to the right)? load(&#39;skew.RData&#39;) boxplot(dat) hist(dat[,4]) # Column 4 has a positive skew. Notice that boxplot function automatically splits each column of the dat (total 9 columns). Hence, there are 9 boxplots total, each of which has 1000 points; the dimension of dat is 1000 x 9 (dim(dat)). Negative skew refers to a longer or fatter tail on the left side of the distribution, while positive skew refers to a longer or fatter tail on the right. Question 4 Which column has negative skew (a long tail to the left)? hist(dat[,9]) # Column 9 has a negative skew. Question 5 Let’s consider a random sample of finishers from the New York City Marathon in 2002. This dataset can be found in the UsingR package. Load the library and then load the nym.2002 dataset. library(dplyr) data(nym.2002, package=&quot;UsingR&quot;) Use boxplots and histograms to compare the finishing times of males and females. Which of the following best describes the difference? data(nym.2002, package=&quot;UsingR&quot;) male &lt;- nym.2002 %&gt;% filter(gender == &#39;Male&#39;) female &lt;- nym.2002 %&gt;% filter(gender == &#39;Female&#39;) mypar(1,2) hist(female$time, xlim = c(100,600)) hist(male$time, xlim = c(100,600)) Both histograms have a similar distribution (skewed to the right). But the center of the histogram seems to be different. We can check this by calculating the absolute difference of the mean and median. abs(mean(male$time) - mean(female$time)) ## [1] 23.11574 abs(median(male$time) - median(female$time)) ## [1] 21.70833 There is a difference of around 21-23 minutes between males and females. So answer C seems to be appropriate: Males and females have similar right skewed distributions, with the former 20 minutes shifted to the left. Question 6 Use dplyr to create two new data frames: males and females, with the data for each gender. For males, what is the Pearson correlation between age and time to finish? plot(male$age, male$time, main = &#39;male&#39;) cor(male$age, male$time) ## [1] 0.2432273 Question 7 For females, what is the Pearson correlation between age and time to finish? plot(female$age, female$time, main = &#39;female&#39;) cor(female$age, female$time) ## [1] 0.2443156 Question 8 If we interpret these correlations without visualizing the data, we would conclude that the older we get, the slower we run marathons, regardless of gender. Look at scatterplots and boxplots of times stratified by age groups (20-24, 25-30, etc.). After examining the data, what is a more reasonable conclusion? groups_m &lt;- split(male$time, floor(male$age/5)*5) # 10-14, 15-19, etc groups_f &lt;- split(female$time, floor(female$age/5)*5) # 10-14, 15-19, etc mypar(1,2) boxplot(groups_m) boxplot(groups_f) This is a tricky question because the question asks you to stratify the data into groups. Stratification can be achieved via split function. To have each group a range of 5 (ex. 25-30), all the age numbers will have to be rounded up or down so that the resulting numbers will be divisible by 5. I rounded the numbers down by using the floor function. As a result, 40 represents the 40-44 age group. You can also use the ceiling function to stratify the data, which will then be rounded up. So, 45 represents 41-45 age group. In the example below, age of 42 is categorized using both floor and ceiling functions. floor(42/5)*5 ## [1] 40 ceiling(42/5)*5 ## [1] 45 The appropriate answer is A: Finish times are constant up until about our 40s, then we get slower. Question 9 When is it appropriate to use pie charts or donut charts? Never (answer choice D) Question 10 The use of pseudo-3D plots in the literature mostly adds: Confusion (answer choice D) 3.11 Exercises First load the data. data(ChickWeight) mypar() plot(ChickWeight$Time, ChickWeight$weight, col=ChickWeight$Diet) chick = reshape(ChickWeight, idvar=c(&quot;Chick&quot;,&quot;Diet&quot;), timevar=&quot;Time&quot;, direction=&quot;wide&quot;) chick = na.omit(chick) Question 1 Focus on the chick weights on day 4 (check the column names of chick and note the numbers). How much does the average of chick weights at day 4 increase if we add an outlier measurement of 3000 grams? Specifically, what is the average weight of the day 4 chicks, including the outlier chick, divided by the average of the weight of the day 4 chicks without the outlier. Hint: use c to add a number to a vector. chick_w4 &lt;- chick[,&#39;weight.4&#39;] chick_w4_add &lt;- append(chick_w4, 3000) # or use function c # chick_w4_add &lt;- c(chick_w4, 3000) chick_w4_add ## [1] 59 58 55 56 48 59 57 59 52 63 56 53 ## [13] 62 61 55 54 62 64 61 58 62 57 58 58 ## [25] 59 59 62 65 63 63 64 61 56 61 61 66 ## [37] 66 63 69 61 62 66 62 64 67 3000 mean(chick_w4_add) - mean(chick_w4) # Difference between with and without outlier ## [1] 63.90966 mean(chick_w4_add)/mean(chick_w4) # Ratio between with and without outlier ## [1] 2.062407 Question 2 In exercise 1, we saw how sensitive the mean is to outliers. Now let’s see what happens when we use the median instead of the mean. Compute the same ratio, but now using median instead of mean. Specifically, what is the median weight of the day 4 chicks, including the outlier chick, divided by the median of the weight of the day 4 chicks without the outlier. median(chick_w4_add) - median(chick_w4) # difference ## [1] 0 median(chick_w4_add)/median(chick_w4) # ratio ## [1] 1 Question 3 Now try the same thing with the sample standard deviation (the sd function in R). Add a chick with weight 3000 grams to the chick weights from day 4. How much does the standard deviation change? What’s the standard deviation with the outlier chick divided by the standard deviation without the outlier chick? sd(chick_w4_add) - sd(chick_w4) # difference ## [1] 429.1973 sd(chick_w4_add)/ sd(chick_w4) # ratio ## [1] 101.2859 Question 4 Compare the result above to the median absolute deviation in R, which is calculated with the mad function. Note that the mad is unaffected by the addition of a single outlier. The mad function in R includes the scaling factor 1.4826, such that mad and sd are very similar for a sample from a normal distribution. What’s the MAD with the outlier chick divided by the MAD without the outlier chick? mad(chick_w4_add) - mad(chick_w4) # difference ## [1] 0 mad(chick_w4_add)/ mad(chick_w4) # ratio ## [1] 1 Question 5 Our last question relates to how the Pearson correlation is affected by an outlier as compared to the Spearman correlation. The Pearson correlation between x and y is given in R by cor(x,y). The Spearman correlation is given by cor(x,y,method=\"spearman\"). Plot the weights of chicks from day 4 and day 21. We can see that there is some general trend, with the lower weight chicks on day 4 having low weight again on day 21, and likewise for the high weight chicks. Calculate the Pearson correlation of the weights of chicks from day 4 and day 21. Now calculate how much the Pearson correlation changes if we add a chick that weighs 3000 on day 4 and 3000 on day 21. Again, divide the Pearson correlation with the outlier chick over the Pearson correlation computed without the outliers. chick_w21 &lt;- chick[, &#39;weight.21&#39;] chick_w21 ## [1] 205 215 202 157 223 157 305 98 124 175 205 96 266 142 157 ## [16] 117 331 167 175 74 265 251 192 233 309 150 256 305 147 341 ## [31] 373 220 178 290 272 321 204 281 200 196 238 205 322 237 264 plot(chick_w4, chick_w21) cor(chick_w4,chick_w21) # correlation before ## [1] 0.4159499 chick_w21_add &lt;- append(chick_w21, 3000) cor(chick_w4_add, chick_w21_add) # correlation after outlier ## [1] 0.9861002 cor(chick_w4_add, chick_w21_add)/cor(chick_w4,chick_w21) # ratio between after and before ## [1] 2.370719 Question 6 Save the weights of the chicks on day 4 from diet 1 as a vector x. Save the weights of the chicks on day 4 from diet 4 as a vector y. Perform a t-test comparing x and y(in R the function t.test(x,y) will perform the test). Then perform a Wilcoxon test of x and y (in R the function wilcox.test(x,y) will perform the test). A warning will appear that an exact p-value cannot be calculated with ties, so an approximation is used, which is fine for our purposes. Perform a t-test of x and y, after adding a single chick of weight 200 grams to x (the diet 1 chicks). What is the p-value from this test? The p-value of a test is available with the following code: t.test(x,y)$p.value x &lt;- chick %&gt;% filter(Diet == 1) x &lt;- x[,&#39;weight.4&#39;] y &lt;- chick %&gt;% filter(Diet == 4) y &lt;- y[,&#39;weight.4&#39;] t.test(x,y)$p.value # t.test result with no outlier ## [1] 7.320259e-06 wilcox.test(x,y)$p.value # wilcox result with no outlier ## Warning in wilcox.test.default(x, y): cannot compute exact p- ## value with ties ## [1] 0.0002011939 x_add &lt;- c(x,200) # outlier added t.test(x_add,y)$p.value # t-test after outlier ## [1] 0.9380347 Question 7 Do the same for the Wilcoxon test. The Wilcoxon test is robust to the outlier. In addition, it has fewer assumptions than the t-test on the distribution of the underlying data. wilcox.test(x_add,y)$p.value # even with outlier, p-value is not perturbed ## Warning in wilcox.test.default(x_add, y): cannot compute exact p- ## value with ties ## [1] 0.0009840921 Question 8 We will now investigate a possible downside to the Wilcoxon-Mann-Whitney test statistic. Using the following code to make three boxplots, showing the true Diet 1 vs 4 weights, and then two altered versions: one with an additional difference of 10 grams and one with an additional difference of 100 grams. Use the x and y as defined above, NOT the ones with the added outlier. library(rafalib) mypar(1,3) boxplot(x,y) boxplot(x,y+10) boxplot(x,y+100) What is the difference in t-test statistic (obtained by t.test(x,y)$statistic) between adding 10 and adding 100 to all the values in the group y? Take the the t-test statistic with x and y+10 and subtract the t-test statistic with x and y+100. The value should be positive. t.test(x,y+10)$statistic - t.test(x,y+100)$statistic ## t ## 67.75097 Question 9 Examine the Wilcoxon test statistic for x and y+10 and for x and y+100. Because the Wilcoxon works on ranks, once the two groups show complete separation, that is, all points from group y are above all points from group x, the statistic will not change, regardless of how large the difference grows. Likewise, the p-value has a minimum value, regardless of how far apart the groups are. This means that the Wilcoxon test can be considered less powerful than the t-test in certain contexts. In fact, for small sample sizes, the p-value can’t be very small, even when the difference is very large. What is the p-value if we compare c(1,2,3) to c(4,5,6) using a Wilcoxon test? wilcox.test(x,y+10)$p.value ## Warning in wilcox.test.default(x, y + 10): cannot compute exact ## p-value with ties ## [1] 5.032073e-05 wilcox.test(x,y+100)$p.value ## Warning in wilcox.test.default(x, y + 100): cannot compute exact ## p-value with ties ## [1] 5.032073e-05 wilcox.test(c(1,2,3),c(4,5,6))$p.value # Answer ## [1] 0.1 Question 10 What is the p-value if we compare c(1,2,3) to c(400,500,600) using a Wilcoxon test? wilcox.test(c(1,2,3),c(400,500,600))$p.value ## [1] 0.1 "],
["matrix-algebra.html", "Chapter 4 Matrix Algebra 4.2 Exercises 4.6 Exercises 4.8 Exercises 4.10 Exercises", " Chapter 4 Matrix Algebra Note: I have rephrased some parts of the questions for clarity. These changes are bolded. Due to the random numbers, the exact values of the answers, despite the same seeds, might differ. So please be mindful of that. First, upload necessary package(s). library(dplyr) # uplaods the function filter() and %&gt;% library(rafalib) # important for plotting with base R 4.2 Exercises #install.packages(&#39;UsingR&#39;) data(&#39;father.son&#39;, package = &#39;UsingR&#39;) Question 1 What is the average height of the sons (don’t round off)? y &lt;- father.son$sheight # son x &lt;- father.son$fheight # father mean(y) ## [1] 68.68407 Question 2 One of the defining features of regression is that we stratify one variable based on others. In statistics, we use the verb ‘condition.’ For exmaple,, the linear model for son and father heights answers the question: how tall do I expect a son to be if I condition on his father being \\(x\\) inches? The regression line answers this question for any \\(x\\). Using the father.son dataset described above, we want to know the expected height of sons, if we condition on the father being 71 inches. Create a list of son heights for sons that have fathers with heights of 71 inches, rounding to the nearest inch. What is the mean of the son heights for fathers that have a height of 71 inches (don’t round off your answer)? Hint: use the function round on the fathers’ heights. groups &lt;- split(y, round(x)) mean(groups[&#39;71&#39;] %&gt;% unlist()) ## [1] 70.54082 Question 3 of the following cannot be written as a linear model? The answer is C: \\(Y = a + b^t + \\epsilon\\). This is because the variable \\(t\\) is an exponent unlike all the other ansewr choices. Question 4 Suppose you model the relationship between weight and height across individuals with a linear model. You assume that the height of individuals for a fixed weight \\(x\\) follows a linear model \\(Y = a + bx + \\epsilon\\). Which of the following do you feel best describes what \\(\\epsilon\\) represents? The answer is D: Between individual variability: people of the same height vary in their weight. Let’s look at each answer choice. Let’s first think \\(Y\\) as son’s height and \\(x\\) as father’s height. So in this linear model, our goal is to predict son’s height based on father’s height. Choice A (Measurement error: scales are not perfect) seems not wrong. However, if the father’s height is 71 inches, can we gurantee that the son’s height is a certain number with a small measurement variability? This is not true because the son’s range of height can still be wide even if the father’s height is 71 inches. In fact, siblings can have very different heights even if their biological parents are identical. Since choice A only describes the \\(\\epsilon\\) as measurement variability, the description seems inadequate. This explanation also applies to Choice B (Within individual random fluctuations: you don’t weigh the same in the morning as in the afternoon) and C (Round off error introduced by the computer). Therefore, choice D seems to be most appropriate. 4.6 Exercises Question 1 In R we have vectors and matrices. You can create your own vectors with the function c. c(1,5,3,4) ## [1] 1 5 3 4 They are also the output of many functions such as: rnorm(10) ## [1] -0.8043316 -1.0565257 -1.0353958 -1.1855604 -0.5004395 ## [6] -0.5249887 -0.3024330 0.4719681 -0.2483839 1.2593180 You can turn vectors into matrcies using functions such as rbind, cbind, or matrix. Create the matrix from the vector 1:1000 like this: X = matrix(1:1000,100,10) What is the entry in row 25, column 3? X[25,3] ## [1] 225 Question 2 Using the function cbind, create a 10 x 5 matrix with first column x=1:10. Then add 2*x, 3*x, 4*x and 5*x to columns 2 through 5. What is the sum of the elements of the 7th row? first_column &lt;- 1:10 y &lt;- cbind(x1=first_column, x2=first_column*2, x3=first_column*3, x4=first_column*4, x5=first_column*5) sum(y[7,]) ## [1] 105 Question 3 Which of the following creates a matrix with multiples of 3 in the third column? matrix(1:60,20,3) # choice A ## [,1] [,2] [,3] ## [1,] 1 21 41 ## [2,] 2 22 42 ## [3,] 3 23 43 ## [4,] 4 24 44 ## [5,] 5 25 45 ## [6,] 6 26 46 ## [7,] 7 27 47 ## [8,] 8 28 48 ## [9,] 9 29 49 ## [10,] 10 30 50 ## [11,] 11 31 51 ## [12,] 12 32 52 ## [13,] 13 33 53 ## [14,] 14 34 54 ## [15,] 15 35 55 ## [16,] 16 36 56 ## [17,] 17 37 57 ## [18,] 18 38 58 ## [19,] 19 39 59 ## [20,] 20 40 60 matrix(1:60,20,3, byrow=T) # choice B ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## [4,] 10 11 12 ## [5,] 13 14 15 ## [6,] 16 17 18 ## [7,] 19 20 21 ## [8,] 22 23 24 ## [9,] 25 26 27 ## [10,] 28 29 30 ## [11,] 31 32 33 ## [12,] 34 35 36 ## [13,] 37 38 39 ## [14,] 40 41 42 ## [15,] 43 44 45 ## [16,] 46 47 48 ## [17,] 49 50 51 ## [18,] 52 53 54 ## [19,] 55 56 57 ## [20,] 58 59 60 x = 11:20; rbind(x,2*x,3*x) # choice C ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## x 11 12 13 14 15 16 17 18 19 20 ## 22 24 26 28 30 32 34 36 38 40 ## 33 36 39 42 45 48 51 54 57 60 x = 1:40; matrix(3*x,20,2) # choice D ## [,1] [,2] ## [1,] 3 63 ## [2,] 6 66 ## [3,] 9 69 ## [4,] 12 72 ## [5,] 15 75 ## [6,] 18 78 ## [7,] 21 81 ## [8,] 24 84 ## [9,] 27 87 ## [10,] 30 90 ## [11,] 33 93 ## [12,] 36 96 ## [13,] 39 99 ## [14,] 42 102 ## [15,] 45 105 ## [16,] 48 108 ## [17,] 51 111 ## [18,] 54 114 ## [19,] 57 117 ## [20,] 60 120 The answer is B. 4.8 Exercises Question 1 Suppose \\(X\\) is a matrix in R. Which of the following is not equivalent to \\(X\\)? t(t(X)) # A ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 ## [7,] 7 107 207 307 407 507 607 707 807 907 ## [8,] 8 108 208 308 408 508 608 708 808 908 ## [9,] 9 109 209 309 409 509 609 709 809 909 ## [10,] 10 110 210 310 410 510 610 710 810 910 ## [11,] 11 111 211 311 411 511 611 711 811 911 ## [12,] 12 112 212 312 412 512 612 712 812 912 ## [13,] 13 113 213 313 413 513 613 713 813 913 ## [14,] 14 114 214 314 414 514 614 714 814 914 ## [15,] 15 115 215 315 415 515 615 715 815 915 ## [16,] 16 116 216 316 416 516 616 716 816 916 ## [17,] 17 117 217 317 417 517 617 717 817 917 ## [18,] 18 118 218 318 418 518 618 718 818 918 ## [19,] 19 119 219 319 419 519 619 719 819 919 ## [20,] 20 120 220 320 420 520 620 720 820 920 ## [21,] 21 121 221 321 421 521 621 721 821 921 ## [22,] 22 122 222 322 422 522 622 722 822 922 ## [23,] 23 123 223 323 423 523 623 723 823 923 ## [24,] 24 124 224 324 424 524 624 724 824 924 ## [25,] 25 125 225 325 425 525 625 725 825 925 ## [26,] 26 126 226 326 426 526 626 726 826 926 ## [27,] 27 127 227 327 427 527 627 727 827 927 ## [28,] 28 128 228 328 428 528 628 728 828 928 ## [29,] 29 129 229 329 429 529 629 729 829 929 ## [30,] 30 130 230 330 430 530 630 730 830 930 ## [31,] 31 131 231 331 431 531 631 731 831 931 ## [32,] 32 132 232 332 432 532 632 732 832 932 ## [33,] 33 133 233 333 433 533 633 733 833 933 ## [34,] 34 134 234 334 434 534 634 734 834 934 ## [35,] 35 135 235 335 435 535 635 735 835 935 ## [36,] 36 136 236 336 436 536 636 736 836 936 ## [37,] 37 137 237 337 437 537 637 737 837 937 ## [38,] 38 138 238 338 438 538 638 738 838 938 ## [39,] 39 139 239 339 439 539 639 739 839 939 ## [40,] 40 140 240 340 440 540 640 740 840 940 ## [41,] 41 141 241 341 441 541 641 741 841 941 ## [42,] 42 142 242 342 442 542 642 742 842 942 ## [43,] 43 143 243 343 443 543 643 743 843 943 ## [44,] 44 144 244 344 444 544 644 744 844 944 ## [45,] 45 145 245 345 445 545 645 745 845 945 ## [46,] 46 146 246 346 446 546 646 746 846 946 ## [47,] 47 147 247 347 447 547 647 747 847 947 ## [48,] 48 148 248 348 448 548 648 748 848 948 ## [49,] 49 149 249 349 449 549 649 749 849 949 ## [50,] 50 150 250 350 450 550 650 750 850 950 ## [51,] 51 151 251 351 451 551 651 751 851 951 ## [52,] 52 152 252 352 452 552 652 752 852 952 ## [53,] 53 153 253 353 453 553 653 753 853 953 ## [54,] 54 154 254 354 454 554 654 754 854 954 ## [55,] 55 155 255 355 455 555 655 755 855 955 ## [56,] 56 156 256 356 456 556 656 756 856 956 ## [57,] 57 157 257 357 457 557 657 757 857 957 ## [58,] 58 158 258 358 458 558 658 758 858 958 ## [59,] 59 159 259 359 459 559 659 759 859 959 ## [60,] 60 160 260 360 460 560 660 760 860 960 ## [61,] 61 161 261 361 461 561 661 761 861 961 ## [62,] 62 162 262 362 462 562 662 762 862 962 ## [63,] 63 163 263 363 463 563 663 763 863 963 ## [64,] 64 164 264 364 464 564 664 764 864 964 ## [65,] 65 165 265 365 465 565 665 765 865 965 ## [66,] 66 166 266 366 466 566 666 766 866 966 ## [67,] 67 167 267 367 467 567 667 767 867 967 ## [68,] 68 168 268 368 468 568 668 768 868 968 ## [69,] 69 169 269 369 469 569 669 769 869 969 ## [70,] 70 170 270 370 470 570 670 770 870 970 ## [71,] 71 171 271 371 471 571 671 771 871 971 ## [72,] 72 172 272 372 472 572 672 772 872 972 ## [73,] 73 173 273 373 473 573 673 773 873 973 ## [74,] 74 174 274 374 474 574 674 774 874 974 ## [75,] 75 175 275 375 475 575 675 775 875 975 ## [76,] 76 176 276 376 476 576 676 776 876 976 ## [77,] 77 177 277 377 477 577 677 777 877 977 ## [78,] 78 178 278 378 478 578 678 778 878 978 ## [79,] 79 179 279 379 479 579 679 779 879 979 ## [80,] 80 180 280 380 480 580 680 780 880 980 ## [81,] 81 181 281 381 481 581 681 781 881 981 ## [82,] 82 182 282 382 482 582 682 782 882 982 ## [83,] 83 183 283 383 483 583 683 783 883 983 ## [84,] 84 184 284 384 484 584 684 784 884 984 ## [85,] 85 185 285 385 485 585 685 785 885 985 ## [86,] 86 186 286 386 486 586 686 786 886 986 ## [87,] 87 187 287 387 487 587 687 787 887 987 ## [88,] 88 188 288 388 488 588 688 788 888 988 ## [89,] 89 189 289 389 489 589 689 789 889 989 ## [90,] 90 190 290 390 490 590 690 790 890 990 ## [91,] 91 191 291 391 491 591 691 791 891 991 ## [92,] 92 192 292 392 492 592 692 792 892 992 ## [93,] 93 193 293 393 493 593 693 793 893 993 ## [94,] 94 194 294 394 494 594 694 794 894 994 ## [95,] 95 195 295 395 495 595 695 795 895 995 ## [96,] 96 196 296 396 496 596 696 796 896 996 ## [97,] 97 197 297 397 497 597 697 797 897 997 ## [98,] 98 198 298 398 498 598 698 798 898 998 ## [99,] 99 199 299 399 499 599 699 799 899 999 ## [100,] 100 200 300 400 500 600 700 800 900 1000 X %*% matrix(1,ncol(X)) # B ## [,1] ## [1,] 4510 ## [2,] 4520 ## [3,] 4530 ## [4,] 4540 ## [5,] 4550 ## [6,] 4560 ## [7,] 4570 ## [8,] 4580 ## [9,] 4590 ## [10,] 4600 ## [11,] 4610 ## [12,] 4620 ## [13,] 4630 ## [14,] 4640 ## [15,] 4650 ## [16,] 4660 ## [17,] 4670 ## [18,] 4680 ## [19,] 4690 ## [20,] 4700 ## [21,] 4710 ## [22,] 4720 ## [23,] 4730 ## [24,] 4740 ## [25,] 4750 ## [26,] 4760 ## [27,] 4770 ## [28,] 4780 ## [29,] 4790 ## [30,] 4800 ## [31,] 4810 ## [32,] 4820 ## [33,] 4830 ## [34,] 4840 ## [35,] 4850 ## [36,] 4860 ## [37,] 4870 ## [38,] 4880 ## [39,] 4890 ## [40,] 4900 ## [41,] 4910 ## [42,] 4920 ## [43,] 4930 ## [44,] 4940 ## [45,] 4950 ## [46,] 4960 ## [47,] 4970 ## [48,] 4980 ## [49,] 4990 ## [50,] 5000 ## [51,] 5010 ## [52,] 5020 ## [53,] 5030 ## [54,] 5040 ## [55,] 5050 ## [56,] 5060 ## [57,] 5070 ## [58,] 5080 ## [59,] 5090 ## [60,] 5100 ## [61,] 5110 ## [62,] 5120 ## [63,] 5130 ## [64,] 5140 ## [65,] 5150 ## [66,] 5160 ## [67,] 5170 ## [68,] 5180 ## [69,] 5190 ## [70,] 5200 ## [71,] 5210 ## [72,] 5220 ## [73,] 5230 ## [74,] 5240 ## [75,] 5250 ## [76,] 5260 ## [77,] 5270 ## [78,] 5280 ## [79,] 5290 ## [80,] 5300 ## [81,] 5310 ## [82,] 5320 ## [83,] 5330 ## [84,] 5340 ## [85,] 5350 ## [86,] 5360 ## [87,] 5370 ## [88,] 5380 ## [89,] 5390 ## [90,] 5400 ## [91,] 5410 ## [92,] 5420 ## [93,] 5430 ## [94,] 5440 ## [95,] 5450 ## [96,] 5460 ## [97,] 5470 ## [98,] 5480 ## [99,] 5490 ## [100,] 5500 X*1 # C ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 ## [7,] 7 107 207 307 407 507 607 707 807 907 ## [8,] 8 108 208 308 408 508 608 708 808 908 ## [9,] 9 109 209 309 409 509 609 709 809 909 ## [10,] 10 110 210 310 410 510 610 710 810 910 ## [11,] 11 111 211 311 411 511 611 711 811 911 ## [12,] 12 112 212 312 412 512 612 712 812 912 ## [13,] 13 113 213 313 413 513 613 713 813 913 ## [14,] 14 114 214 314 414 514 614 714 814 914 ## [15,] 15 115 215 315 415 515 615 715 815 915 ## [16,] 16 116 216 316 416 516 616 716 816 916 ## [17,] 17 117 217 317 417 517 617 717 817 917 ## [18,] 18 118 218 318 418 518 618 718 818 918 ## [19,] 19 119 219 319 419 519 619 719 819 919 ## [20,] 20 120 220 320 420 520 620 720 820 920 ## [21,] 21 121 221 321 421 521 621 721 821 921 ## [22,] 22 122 222 322 422 522 622 722 822 922 ## [23,] 23 123 223 323 423 523 623 723 823 923 ## [24,] 24 124 224 324 424 524 624 724 824 924 ## [25,] 25 125 225 325 425 525 625 725 825 925 ## [26,] 26 126 226 326 426 526 626 726 826 926 ## [27,] 27 127 227 327 427 527 627 727 827 927 ## [28,] 28 128 228 328 428 528 628 728 828 928 ## [29,] 29 129 229 329 429 529 629 729 829 929 ## [30,] 30 130 230 330 430 530 630 730 830 930 ## [31,] 31 131 231 331 431 531 631 731 831 931 ## [32,] 32 132 232 332 432 532 632 732 832 932 ## [33,] 33 133 233 333 433 533 633 733 833 933 ## [34,] 34 134 234 334 434 534 634 734 834 934 ## [35,] 35 135 235 335 435 535 635 735 835 935 ## [36,] 36 136 236 336 436 536 636 736 836 936 ## [37,] 37 137 237 337 437 537 637 737 837 937 ## [38,] 38 138 238 338 438 538 638 738 838 938 ## [39,] 39 139 239 339 439 539 639 739 839 939 ## [40,] 40 140 240 340 440 540 640 740 840 940 ## [41,] 41 141 241 341 441 541 641 741 841 941 ## [42,] 42 142 242 342 442 542 642 742 842 942 ## [43,] 43 143 243 343 443 543 643 743 843 943 ## [44,] 44 144 244 344 444 544 644 744 844 944 ## [45,] 45 145 245 345 445 545 645 745 845 945 ## [46,] 46 146 246 346 446 546 646 746 846 946 ## [47,] 47 147 247 347 447 547 647 747 847 947 ## [48,] 48 148 248 348 448 548 648 748 848 948 ## [49,] 49 149 249 349 449 549 649 749 849 949 ## [50,] 50 150 250 350 450 550 650 750 850 950 ## [51,] 51 151 251 351 451 551 651 751 851 951 ## [52,] 52 152 252 352 452 552 652 752 852 952 ## [53,] 53 153 253 353 453 553 653 753 853 953 ## [54,] 54 154 254 354 454 554 654 754 854 954 ## [55,] 55 155 255 355 455 555 655 755 855 955 ## [56,] 56 156 256 356 456 556 656 756 856 956 ## [57,] 57 157 257 357 457 557 657 757 857 957 ## [58,] 58 158 258 358 458 558 658 758 858 958 ## [59,] 59 159 259 359 459 559 659 759 859 959 ## [60,] 60 160 260 360 460 560 660 760 860 960 ## [61,] 61 161 261 361 461 561 661 761 861 961 ## [62,] 62 162 262 362 462 562 662 762 862 962 ## [63,] 63 163 263 363 463 563 663 763 863 963 ## [64,] 64 164 264 364 464 564 664 764 864 964 ## [65,] 65 165 265 365 465 565 665 765 865 965 ## [66,] 66 166 266 366 466 566 666 766 866 966 ## [67,] 67 167 267 367 467 567 667 767 867 967 ## [68,] 68 168 268 368 468 568 668 768 868 968 ## [69,] 69 169 269 369 469 569 669 769 869 969 ## [70,] 70 170 270 370 470 570 670 770 870 970 ## [71,] 71 171 271 371 471 571 671 771 871 971 ## [72,] 72 172 272 372 472 572 672 772 872 972 ## [73,] 73 173 273 373 473 573 673 773 873 973 ## [74,] 74 174 274 374 474 574 674 774 874 974 ## [75,] 75 175 275 375 475 575 675 775 875 975 ## [76,] 76 176 276 376 476 576 676 776 876 976 ## [77,] 77 177 277 377 477 577 677 777 877 977 ## [78,] 78 178 278 378 478 578 678 778 878 978 ## [79,] 79 179 279 379 479 579 679 779 879 979 ## [80,] 80 180 280 380 480 580 680 780 880 980 ## [81,] 81 181 281 381 481 581 681 781 881 981 ## [82,] 82 182 282 382 482 582 682 782 882 982 ## [83,] 83 183 283 383 483 583 683 783 883 983 ## [84,] 84 184 284 384 484 584 684 784 884 984 ## [85,] 85 185 285 385 485 585 685 785 885 985 ## [86,] 86 186 286 386 486 586 686 786 886 986 ## [87,] 87 187 287 387 487 587 687 787 887 987 ## [88,] 88 188 288 388 488 588 688 788 888 988 ## [89,] 89 189 289 389 489 589 689 789 889 989 ## [90,] 90 190 290 390 490 590 690 790 890 990 ## [91,] 91 191 291 391 491 591 691 791 891 991 ## [92,] 92 192 292 392 492 592 692 792 892 992 ## [93,] 93 193 293 393 493 593 693 793 893 993 ## [94,] 94 194 294 394 494 594 694 794 894 994 ## [95,] 95 195 295 395 495 595 695 795 895 995 ## [96,] 96 196 296 396 496 596 696 796 896 996 ## [97,] 97 197 297 397 497 597 697 797 897 997 ## [98,] 98 198 298 398 498 598 698 798 898 998 ## [99,] 99 199 299 399 499 599 699 799 899 999 ## [100,] 100 200 300 400 500 600 700 800 900 1000 X %*% diag(ncol(X)) # D ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 1 101 201 301 401 501 601 701 801 901 ## [2,] 2 102 202 302 402 502 602 702 802 902 ## [3,] 3 103 203 303 403 503 603 703 803 903 ## [4,] 4 104 204 304 404 504 604 704 804 904 ## [5,] 5 105 205 305 405 505 605 705 805 905 ## [6,] 6 106 206 306 406 506 606 706 806 906 ## [7,] 7 107 207 307 407 507 607 707 807 907 ## [8,] 8 108 208 308 408 508 608 708 808 908 ## [9,] 9 109 209 309 409 509 609 709 809 909 ## [10,] 10 110 210 310 410 510 610 710 810 910 ## [11,] 11 111 211 311 411 511 611 711 811 911 ## [12,] 12 112 212 312 412 512 612 712 812 912 ## [13,] 13 113 213 313 413 513 613 713 813 913 ## [14,] 14 114 214 314 414 514 614 714 814 914 ## [15,] 15 115 215 315 415 515 615 715 815 915 ## [16,] 16 116 216 316 416 516 616 716 816 916 ## [17,] 17 117 217 317 417 517 617 717 817 917 ## [18,] 18 118 218 318 418 518 618 718 818 918 ## [19,] 19 119 219 319 419 519 619 719 819 919 ## [20,] 20 120 220 320 420 520 620 720 820 920 ## [21,] 21 121 221 321 421 521 621 721 821 921 ## [22,] 22 122 222 322 422 522 622 722 822 922 ## [23,] 23 123 223 323 423 523 623 723 823 923 ## [24,] 24 124 224 324 424 524 624 724 824 924 ## [25,] 25 125 225 325 425 525 625 725 825 925 ## [26,] 26 126 226 326 426 526 626 726 826 926 ## [27,] 27 127 227 327 427 527 627 727 827 927 ## [28,] 28 128 228 328 428 528 628 728 828 928 ## [29,] 29 129 229 329 429 529 629 729 829 929 ## [30,] 30 130 230 330 430 530 630 730 830 930 ## [31,] 31 131 231 331 431 531 631 731 831 931 ## [32,] 32 132 232 332 432 532 632 732 832 932 ## [33,] 33 133 233 333 433 533 633 733 833 933 ## [34,] 34 134 234 334 434 534 634 734 834 934 ## [35,] 35 135 235 335 435 535 635 735 835 935 ## [36,] 36 136 236 336 436 536 636 736 836 936 ## [37,] 37 137 237 337 437 537 637 737 837 937 ## [38,] 38 138 238 338 438 538 638 738 838 938 ## [39,] 39 139 239 339 439 539 639 739 839 939 ## [40,] 40 140 240 340 440 540 640 740 840 940 ## [41,] 41 141 241 341 441 541 641 741 841 941 ## [42,] 42 142 242 342 442 542 642 742 842 942 ## [43,] 43 143 243 343 443 543 643 743 843 943 ## [44,] 44 144 244 344 444 544 644 744 844 944 ## [45,] 45 145 245 345 445 545 645 745 845 945 ## [46,] 46 146 246 346 446 546 646 746 846 946 ## [47,] 47 147 247 347 447 547 647 747 847 947 ## [48,] 48 148 248 348 448 548 648 748 848 948 ## [49,] 49 149 249 349 449 549 649 749 849 949 ## [50,] 50 150 250 350 450 550 650 750 850 950 ## [51,] 51 151 251 351 451 551 651 751 851 951 ## [52,] 52 152 252 352 452 552 652 752 852 952 ## [53,] 53 153 253 353 453 553 653 753 853 953 ## [54,] 54 154 254 354 454 554 654 754 854 954 ## [55,] 55 155 255 355 455 555 655 755 855 955 ## [56,] 56 156 256 356 456 556 656 756 856 956 ## [57,] 57 157 257 357 457 557 657 757 857 957 ## [58,] 58 158 258 358 458 558 658 758 858 958 ## [59,] 59 159 259 359 459 559 659 759 859 959 ## [60,] 60 160 260 360 460 560 660 760 860 960 ## [61,] 61 161 261 361 461 561 661 761 861 961 ## [62,] 62 162 262 362 462 562 662 762 862 962 ## [63,] 63 163 263 363 463 563 663 763 863 963 ## [64,] 64 164 264 364 464 564 664 764 864 964 ## [65,] 65 165 265 365 465 565 665 765 865 965 ## [66,] 66 166 266 366 466 566 666 766 866 966 ## [67,] 67 167 267 367 467 567 667 767 867 967 ## [68,] 68 168 268 368 468 568 668 768 868 968 ## [69,] 69 169 269 369 469 569 669 769 869 969 ## [70,] 70 170 270 370 470 570 670 770 870 970 ## [71,] 71 171 271 371 471 571 671 771 871 971 ## [72,] 72 172 272 372 472 572 672 772 872 972 ## [73,] 73 173 273 373 473 573 673 773 873 973 ## [74,] 74 174 274 374 474 574 674 774 874 974 ## [75,] 75 175 275 375 475 575 675 775 875 975 ## [76,] 76 176 276 376 476 576 676 776 876 976 ## [77,] 77 177 277 377 477 577 677 777 877 977 ## [78,] 78 178 278 378 478 578 678 778 878 978 ## [79,] 79 179 279 379 479 579 679 779 879 979 ## [80,] 80 180 280 380 480 580 680 780 880 980 ## [81,] 81 181 281 381 481 581 681 781 881 981 ## [82,] 82 182 282 382 482 582 682 782 882 982 ## [83,] 83 183 283 383 483 583 683 783 883 983 ## [84,] 84 184 284 384 484 584 684 784 884 984 ## [85,] 85 185 285 385 485 585 685 785 885 985 ## [86,] 86 186 286 386 486 586 686 786 886 986 ## [87,] 87 187 287 387 487 587 687 787 887 987 ## [88,] 88 188 288 388 488 588 688 788 888 988 ## [89,] 89 189 289 389 489 589 689 789 889 989 ## [90,] 90 190 290 390 490 590 690 790 890 990 ## [91,] 91 191 291 391 491 591 691 791 891 991 ## [92,] 92 192 292 392 492 592 692 792 892 992 ## [93,] 93 193 293 393 493 593 693 793 893 993 ## [94,] 94 194 294 394 494 594 694 794 894 994 ## [95,] 95 195 295 395 495 595 695 795 895 995 ## [96,] 96 196 296 396 496 596 696 796 896 996 ## [97,] 97 197 297 397 497 597 697 797 897 997 ## [98,] 98 198 298 398 498 598 698 798 898 998 ## [99,] 99 199 299 399 499 599 699 799 899 999 ## [100,] 100 200 300 400 500 600 700 800 900 1000 The answer is B. In choice A, the trasnposed matrix \\(X\\) gets transposed again, thereby returning to its original matrix \\(X\\). In choice B, the matrix \\(X\\) gets multiplied by 1, which is a scalar. So it will not be changed. In choice D, \\(X\\) gets multiplied by an identity matrix, so \\(X\\) does not change even after the matrix multiplication. Therefore, the answer is B. Question 2 Solve the following system of equations using R: \\[ \\begin{align*} 3a + 4b - 5c + d=10\\\\ 2a + 2b + 2c - d = 5\\\\ a - b + 5c - 5d = 7 \\\\ 5a + 5d = 4 \\end{align*} \\] What is the solution for \\(c\\)? X &lt;- matrix(c(3,4,-5,1,2,2,2,-1,1,-1,5,-5,5,0,0,1),4,4,byrow=T) ans &lt;- solve(X) %*% matrix(c(10,5,7,4),4,1) ans[3] ## [1] -0.8849558 Question 3 Load the following two matrices into R: a &lt;- matrix(1:12, nrow=4) b &lt;- matrix(1:15, nrow=3) What is the value in the 3rd row and the 2nd column of the matrix product of a and b? a &lt;- matrix(1:12, nrow=4) b &lt;- matrix(1:15, nrow=3) c &lt;- a%*% b c[3,2] ## [1] 113 Question 4 Multiply the 3rd row of a with the 2nd column of b, using the element-wise vector multiplication with *. What is the sum of the elements in the resulting vector? sum(a[3,] * b[,2]) ## [1] 113 4.10 Exercises Question 1 Suppose we are analyzing a set of 4 samples. The first two samples are from a treatment group A and the second two samples are from a treatment group B. This design can be represented with a model matrix like so: X &lt;- matrix(c(1,1,1,1,0,0,1,1),nrow=4) rownames(X) &lt;- c(&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;) X ## [,1] [,2] ## a 1 0 ## a 1 0 ## b 1 1 ## b 1 1 Suppose that the fitted parameters for a linear model give us: beta &lt;- c(5,2) Use the matrix multiplication operator, %*%, in R to answer the following questions: What is the fitted value for the A samples? (The fitted Y values.) X &lt;- matrix(c(1,1,1,1,0,0,1,1),nrow=4) rownames(X) &lt;- c(&#39;a&#39;,&#39;a&#39;,&#39;b&#39;,&#39;b&#39;) #beta &lt;- c(5,2) beta &lt;- matrix(c(5,2),nrow =2, ncol=1) # matrix form of beta X[1:2,] %*% beta ## [,1] ## a 5 ## a 5 When I perform matrix multiplication in R %*%, I usually make sure to make all my values in the matrix form. In this case, I rewrote the beta variable in the function of matrix. However, this is not necessary. beta &lt;- c(5,2) works well too. Question 2 What is the fitted value for the B samples? (The fitted Y values.) X[3:4,] %*% beta ## [,1] ## b 7 ## b 7 Question 3 Suppose now we are comparing two treatments B and C to a control group A, each with two samples. This design can be represented with a model matrix like so: X &lt;- matrix(c(1,1,1,1,1,1,0,0,1,1,0,0,0,0,0,0,1,1),nrow=6) rownames(X) &lt;- c(&quot;a&quot;,&quot;a&quot;,&quot;b&quot;,&quot;b&quot;,&quot;c&quot;,&quot;c&quot;) X ## [,1] [,2] [,3] ## a 1 0 0 ## a 1 0 0 ## b 1 1 0 ## b 1 1 0 ## c 1 0 1 ## c 1 0 1 Suppose that the fitted values for the linear model are given by: beta &lt;- c(10,3,-3) What is the fitted values for the B sample? beta &lt;- matrix(c(10,3,-3),nrow = 3) X[3:4,] %*% beta ## [,1] ## b 13 ## b 13 Question 4 What is the fitted values for the C sample? X[5:6,] %*% beta ## [,1] ## c 7 ## c 7 "]
]
